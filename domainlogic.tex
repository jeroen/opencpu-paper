% !TEX root = main.tex

\section{Practices and domain logic of scientific computing}

%The field and software of data analysis are unlike thos in any other domain. Statistical computing has a long history and many intrinsic properties and peculiarities that are relevent when embedding statistical methods into systems. It is my impression that lack of understanding both within and outside the statistics community about what distinguishes scientific computing from other branches of computer science underlies many of the technical problems. 
%At the same time there are enourmous opportunities to facilitate better science and data analysis using modern technogy that might be overlooked due to a disconnect between statisticians and software engineers. 
%The purpose of this section is to provide a high-level introduction to problems, practices and logic of scientific computing.
This section gives a helicopter view of the practices and logic of scientific computing. 
The topics are presented in a somewhat informal and subjective manner, and are not intended to be exhaustive or exclusive. The purpose is to introduce the concepts and problems that are most relevant in the context of this resarch. The reader should get a sense of what makes data analysis unique and why the software landscape is dominated by Domain Specific Languages (DSL). Along the way we highlight some of the major difficulties and pitfalls, which sets the stage for working towards an interface definition in subsequent sections.

\subsection{It starts with data}

The role and shape of \emph{data} is one of the main characteristics that distinguishes scientific computing. In most general purpose programming languages, \emph{data structures} are instances of classes with predefined properties and methods. Data on disk is stored in a database using schemas to enforce structure. This makes inserting and retrieving data reliable and predicable: a table returned by a given \SQL query always contains exactly the same structure with the requested fields. The only varying property between several executions of a query is the number of returned rows. Also the time needed for the database to process the request usually depends only on the amount of records in the database. This provides the foundations to write code that implements all required operations in advance without knowing the content of the data. It also allows for a clear seperation between developers and users. Most applications do not give users direct access to raw data structures. Developers focus in implementing code and designing data structures, whereas users merely execute code.

This paradigm does not work for scientific computing. Developers of statistical packages have relatively little control over the structure, content and quality of the data. Data analysis usually starts with the user supplying a dataset, which is rarely pretty. Real world data come in all shapes, forms and formats. They are messy, have inconsistent structures, and invisible numeric properties. Therefore statistical programming languages define data structures relatively loosely, and instead implement a rich lexicon for interactively inspecting, manipulating, restructuring, subsetting, combining, playing and computing on data. Unlike software operating on well defined data structures, it is nearly impossible to write code that accounts for any scenario and will work for every possible dataset. Many functions and methods are not applicable to every instance of a particular class, or might behave differently based on dynamic properties of the data, such as size, dimensionality or rank. For these reasons there is also less clear of a seperation between developers and users in scientific computing. The data analysis process involves simultaneously debugging of code and data where the user iterates back and forth between manipulating and analyzing the data. Implementations of statistical methods tend to be very flexible with many parameters and settings to specify and customize behavior for the broad range of possible datasets. And still the user might have to go through many steps of cleaning and reshaping to give data the appropriate structure and properties to perform a particular analysis. 

Informal operations and loosely defined data structures are typical characteristics of scientific computing. They give a lot of freedom to implement powerful and flexible operations for data analysis. However they also complicate interfacing of statistical methods. In the context of an interactive \UI, the user iterates from one step to the next using a trial and error strategy. However embedded systems require a degree of type-safety, predictability and consistency to facilitate reliable I/O between components. These features are native to databases or many object orriented languages, but require substantive effort for statistical software. 


\subsection{Functional programming}

Many different programming languages and styles exists, each with their own strengths and weaknesses. Scientific computing languages typically use a functional style of programming, where methods take a role and notation similar to \emph{functions} in mathematics. This has obvious benefits for numerical computing. Statistical methods are written as $y = f(g(x))$ (rather than $y = x.g().f()$ notation), and a functional syntax results in intuitive code for implementing algorithms. 

However most popular general purpose languages take a more imperative and object oriented approach. In many ways, object-oriented programming can be considered the opposite of functional programming \citep{pythonfunctional}. Here methods are invoked on an object and modify the \emph{state} of this particular object. Object-oriented languages typically implement inheritance of fields and methods based on object classes or prototypes. Software engineers tend to prefer this style of programming because it is more powerful to handle complex data structures. The success of object oriented languages has also influenced scientific computing, resulting in multi-paradigm systems. For example, languages such as Julia and R use multiple dispatch to dynamically assign function calls to a particular function based type of arguments. This brings certain object oriented benefits to functional languages, but also complicates scoping and inheritance. 

A comparitive review on programming styles is beyond the scope of this research. But what is relevant to us is how conflicting paradigms might complicate interfacing analysis components. For example, in the context of web services, the \emph{Representational State Transfer} (REST) style described by \cite{fielding2000architectural} is very popular among web developers. A restful API maps URLs to \emph{resources} and http requests are used to modify the \emph{state} of such a resource. The REST style results in simple and elegant HTTP interfaces for object orriented systems. Unfortunately, REST does not map very naturally to the functional paradigm of statistical software. Languages where functions are first class citizens suggest more RPC flavored interfaces, which according to Fielding are by definition not restful. This does not mean that such a component is incompatible with other pieces. As long as components honor the rules of the protocol (i.e. HTTP) they will work together. However conflicting programming styles can be a source of friction for embedded scientific computing. Strongly object-oriented frameworks or developers might require some additional effort to get comfortable with components implementing a more functional paradigm.



\subsection{Graphics}

Another somewhat domain specific feature of scientific computing is native support for graphics. Most statistical software packages include programs to draw plots and charts in some form or another. In contrast to data and functions which are language objects, the graphics device is implemented as a separate output stream. As such, drawings on the canvas are a \emph{side effect} rather than a return value of function calls. This works a bit similar to manipulating DOM elements in a browser using JavaScript. In most interactive statistical software, graphics appear to the user in a seperate window. The state of this graphics device can not easily be stored or serialized as is the case for functions and objects. Most software packages do include functionality to export an \emph{image} of the graphics device to a file using \texttt{png}, \texttt{svg} or \texttt{pdf} format. However this image is merely a snapshot; it does not contain the actual state of the device can not be reloaded for further manipulation.

First class citizenship of graphics is an important part of interfacing scientific computing. However, output containing both data and graphics makes the design of a general purpose API more difficult. A remote function call needs to capture both return value as well as graphical side effects. Furthermore the interface should allow clients to generate graphics without imposing restrictions on the format or format parameters. Users often want to use a simple bitmap format such as \texttt{png} for previewing a graphic, but have the option to export the same graphic to a high quality format such as \texttt{pdf} for publication. And because statistical computation is often expensive and non-deterministic, we can not simply reconstruct a plot from scratch only to retrieve it in another format. Hence the \API needs to incorporate the notion of a graphics device in a way independent of the imaging format. 

\subsection{Numeric properties and missing values}

We already mentioned how loosely defined data structures in scientific computing can impede type safety of data I/O in analysis components. However statistical methods can choke on the actual content of data as well. Sometimes problematic data can easily be spotted, but often it is nearly impossible to detect these ahead of time. Applying statistical procedures to these data will then result in errors, even though the code and structure of the data are perfectly fine. For example, many statistical models build on matrix decompositions that require the data to follow certain numeric properties. The \emph{rank} of a matrix is one such property which measures the nondegenerateness of the system of linear equations. When a matrix $A$ is rank deficient, the equation $Ax=b$ does not have a solution when $b$ does not lie in the range of $A$. Attempting to solve this equation results in similar computational problems as division by zero. Accounting for this ahead of time is nearly impossible because numeric properties are invisible until they are actually calculated. But perhaps just as difficult is explaining the user or software engineer that these errors are not a bug, and can not be fixed. The procedure just does not work for this particular dataset.

Another example of problematic data is presented by \emph{missing} values. Missingness in statistics means that the value of a field is unknown. However, missing data should not be confused with no data or \texttt{null}. Missing values are often \emph{non ignorable}, meaning that the missingness in itself is information that needs to be accounted for in the modeling. A standard textbook example is when we perform a survey asking people about their salary. Because some people might refuse to provide this information, the data contains missing values. However, the missingness is probably not \emph{completely at random}: people with high salaries might be more reluctant to provide this information than people with a median salary. If we would calculate the mean salary from our data ignoring the missing values, the estimate is likely biased. To obtain a more accurate estimate of the average salary, missing values need to be incorporated in the estimation using a more sophisticated model. 

Statistical programming languages can define serveral types of missing or non-finite values such as \texttt{NA}, \texttt{NaN} or \texttt{Inf}. These are implemented as special primitives, which is one of the benefits of a DSL. Statistical functions and methods have special procedures and options to specify how to handle missing values encountered in the data. However the notion of missingness in foreign to most languages and software outside of scientific computing. They are a typical domain-specific phenomenon that can cause technical problems in data exchange with other systems. And like numeric properties, the concept of a values containing no value is likely to cause confusion among developers or users with limited experience in data analysis. However failure to properly incorporate missing values in the data can easily lead to errors or incorrect results, as the example above illustrated.

\subsection{Non deterministic and unpredictable behavior}

Most software applications are expected to produce consistent output in a timely manner, unless something is very wrong. However this does not generally hold for scientific computing. The previous section explained how problematic data can cause exceptions or unexpected results. But many analysis methods are actually non-deterministic or unpredictable by nature.

Statistical algorithms often repeat some calculation until a particular convergence criterion is reached. Starting values and minor fluctions in the data can have showball effect on the course of the algoritm. Therefore several runs can result in wildly varying outcomes and execution times. Moreover, convergence is often not guaranteed: unfortunate input can get a process stuck in a local minimum or send it off into the wrong direction. It is often difficult predict and control such scenarios and edge cases a-priori in the implementation. Monte-carlo techniques are even more unpredictable, because they are specifically designed to behave randomly. For example, MCMC methods use a Markov-Chain to simulate random-walk through a (high-dimensional) space such as a multivariate propbability density. MCMC methods are popular in simulation studies or numeric integration. Each execution of the random walk will yield different outcomes, but eventually things should converge to the value of interest. However due to randomness it is possible that some of the runs or chains get stuck and need to be terminated or disregarded.

The unpredictability aspect severely complicates embedding of statistical software, and can sometimes surprise software engineers assuming deterministic behavior. Methods in scientific computing are rarely 100\% guaranteed to be successful for arbitrary data. Assuming that a procedure will always return timely and consistently because it did so with a test dataset is very dangerous. Within an interactive session the user can simply interrupt a process that seems to have gotten stuck. However for embedded modules, unpredictability needs to be accounted for in the design of the system. At a very minimum, the system should be able to detect and terminate processes that have not completed when some timeout is reached. But preferably we need a layer or meta functionality to control and monitor jobs and executions, either manually or automatically.  This aspect of scientific computing introduces major complications in comparison with embedding types of software that behave more predictably such as a database. 

\subsection{Experimental software}

In scientific computing we often need to deal with inventive, volatile and experimental  software. This is a big cultural difference with many general purpose languages such as \texttt{python}, \texttt{Java}, \texttt{C++} or \texttt{JavaScript}. The latter communities include professional organizations and engineers committed to implementing and maintaining production quality libraries. However most authors of open source statistical software do not have the expertise and resources to meet such standards. Contributed code in languages such as \texttt{R} was often written to accompany a scientific article proposing novell ideas, models, algorithms, or programming techniques. The script or package serves as an illustration or proof of concept of the presented ideas and was implemented by academics or students. The quality of such contributions varies a lot and no active support, maintenance or backward compatibility should be expected from the authors. Sometimes an update to a package radically changes things based on new ideas or insights. Contributed code might need to be tweaked, customized and tailored to fit a particular problem or dataset. In some cases, published code merely serves as an example of a particular technique, and only provides a starting point for an analysis. 

Because traditional data analysis does not really have a notion of production this has never been a major problem. The emphasis in statistical software has always been on innovation rather than continuity. Experimental code is usually good enough for interactive data analysis where it suffices to manually make a script or package work for the dataset at hand. However integrated components require a greater degree of reliability and continuity which introduces a source of technical and cultural friction for embedded scientific computing. Statistical software developers assume that they are at the top of the stack and that the user will manually manage dependencies or debug code to make things work. 
%This will likely not change overnight. Proposals to make statistical software play nicer with other software have been received with little understanding and sympathy within the community. 
Therefore the ability to manage unstable software, facilitate rapid change, sandbox modules and manage failure are important concerns of embedded scientific computing.

\subsection{Interactivity and error handling}

In general purpose languages, run-time errors are typically caused by a bug or some sort of system failure. Exceptions are only raised when the software can not recover and usually result in termination of the process. Error messages contain information such as calling stacks to help the programmer discover where in the code a problem occured. Software engineers go through great trouble to prevent potential problems ahead of time using smart compilers, unit tests, automatic code analysis and continuous intergration. Errors that do arise during production are usually not displayed to the user, but rather the administrator is notified that the system urgently needs attention. The user gets to see an apology at best.

In scientific computing, errors play a very different role. As a consequence of some of the characteristics discussed earlier, interactive debugging is a natural part of the user experience. Errors in statistics do not necessarily indicate a bug in the software, but rather a problem with the data or some interaction of the code and data. The statistician goes back and forth between cleaning, manipulating, modeling, vizualizing and interpreting to study patterns and relations in the data. This interactive debugging of data and code comes down to a lot of trial and error. Problems with missing values, outliers, types, structures, degrees of freedom or numeric properties might not reveal themselves until we try to fit a model or create a plot. Often times, exceptions raised by a statistical methods are a sign that data needs additional parsing, munging or cleaning. This makes error messages an important source of information for the statistician to get to know a dataset and its difficulties. And while debugging we discover the strenghts and limitations of the analysis methods. In practice we often find out that a particular dataset requires us to research or implement additional techniques because the standard tools do not suffice or are inappropriate.

Interactive error handling is one of the reasons that there is no clear distinction between development and production in scientific computing. When interfacing to analysis modules it is important that the role of errors is understood and recognised. An \API must be able to handle exceptions and report error messages to the user, and certainly not crash the system. The role of errors in data analysis is not very well understood outside of our community. Some popular commercial products seem to have propagated the belief that data analysis comes down to applying a magical tool to a dataset. However systems that implement such canned analyses don't do justice to the wide range of methods that statistics has to offer. Support for interactive error handling is therefore a crucial part of data analysis and embedded scientific computing.



\subsection{Security and resource control}

Somewhat related to the above are special needs in terms of security. Most statistical software currently available is primarily designed for interactive use on the local machine. Therefore access control is not considered an issue and the execution environment is entirely unrestricted. However in a context of shared or public embedded services we need to be able to implement security policies to prevent malicious or excessive use of resources. This in itself is not a unique problem. Most scripting languages such as \texttt{php}, \texttt{python} or \texttt{bash} do not enforce any access control and assume security will be implemented on the application level. However there are two domain specific aspects that complicate the problem. 

The first issue is that statistical software can be demanding and greedy with hardware resources. Numerical methods are expensive both in terms of memory and CPU. Fair use policies are not really feasible because excessive use of resources often happens unintentionally. For example, an overly complex model specification or algorithm getting stuck might end up consuming all available memory and cpu until manually terminated. When this happens on the local machine, the user can easily interrupt the process prematurely by sending a \texttt{SIGINT}, i.e. pressing \texttt{CTRL+C} or \texttt{ESC}. However in a shared environment this needs to be regulated by the system. Embedded scientific computing requires technology and policies that can manage and limit memory allocation, cpu cycles, disk space, concurrent processes, network traffic, etc. The degree of flexibility offered by implementation of resource management is an important factor in the scalability of a system. Fine grained control over system resources consumed by individual tasks allows for serving many concurrent users without sacrificing reliabilty. 

The second domain specific security issue is caused by the need for arbitrary code execution. A traditional application security model is based on user role privileges. For example in a standard web application, only a developer or administrator can implement and deploy actual code. The application merely exposes predefined functionality; users are not allowed to execute arbitrary code on the server. Any possibility of code injection is considered a security vulnerability and when found the server is potientially compromised. However as already mentioned, lack of segregation between users and developers in statistics gives limited use to applications that restrict users to predefined scripts and canned services. To support actual data analysis, the user needs access to the full language lexicon to freely explore, debug, model and visualize data. The need for arbitrary code execution disqualifies user based privileges and demands a more sophisticated security model.



\subsection{Reproducible research}

Replication of findings is one of the main principles of the scientific method. In quantitative research, a necessary condition for replication is reproducibiltity of results. The goal of reproducible research is to tie specific instructions to data analysis and experimental data so that scholarship can be recreated, better understood and verified \citep{cranRR}. Even though the ideas of replication are as old as science itself, reproducibility in scientific computing is still in its infancy. Tools are available that assist users in documenting their actions, but to most researchers these are not a natural part of their daily workflow. However the importance of replication in data analysis is increangly recognised and support for reproducibility is becoming more influential in the design of statistical software.

Reproducibility changes what constitutes the main product of an analysis. Rather than solely output and conclusions, we are interested recording and publishing the entire \emph{process}. This includes all data, code and results but also external software that was used arrive at the results. Therefore, reproducibility puts high requirements on software versioning. More than in other domains it is crucial that we diligently archive and administer the precise versions or branches of all scripts, packages, libraries, plugins that were somehow involved in the process. If an analysis involves randomness, it is also important that we keep track of which seeds and random number generators were used. In the current design of statistical software, reproducibility was mostly an afterthought and has te be taken care of manually. In practice it is tedious, expensive and error-prone. There is a lot of room for improvement for systems to incorporate reproducible practices as a natural part of the data analysis process.

Whereas reproducibility in statistics is acknowledged from a transparency/accountability point of view, it has enourmous potential to become much more than that. There are obvious parallels between reproducible research and revision control in source code management systems. Technology for automatic reproducible data analysis could revolutionize scientific collaboration, similar to what \texttt{git} has done for software development. A system that keeps track of each step in the  analysis process like a ``commit" in software versioning would make peer review, forking or follow-up analysis more practical and accessible. A scientific publication would no longer be considered an end product, but rather the starting point of scientific debate. When colleagues, supervisors or reviewers can easily reproduce results, test alternative hypotheses or recycle data, we achieve greater trustworthiness but also multiply return on investment of our work. Finally an open kitchen can help facilitate more natural ways of learning and teaching statistics. Rather than relying on general purpose textbooks with artificial examples, scholars directly study the practices of prominent researchers to understand how methods are applied in the context of data and problems as they appear specifically in their area of interest.