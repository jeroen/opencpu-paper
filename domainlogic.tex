% !TEX root = main.tex

\section{Practices and domain logic of scientific computing}

%The field and software of data analysis are unlike thos in any other domain. Statistical computing has a long history and many intrinsic properties and peculiarities that are relevent when embedding statistical methods into systems. It is my impression that lack of understanding both within and outside the statistics community about what distinguishes scientific computing from other branches of computer science underlies many of the technical problems. 
%At the same time there are enourmous opportunities to facilitate better science and data analysis using modern technogy that might be overlooked due to a disconnect between statisticians and software engineers. 
%The purpose of this section is to provide a high-level introduction to problems, practices and logic of scientific computing.

This section provides a helicopter view of the practices and logic of scientific computing that are most relevant in the context of this research. The reader should get a sense of what is involved with scientific computing, what makes data analysis unique and why the software landscape is dominated by domain specific languages (\texttt{DSL}). The topics are chosen and presented somewhat subjectively based on my experiences in this field. They are not intended to be exhaustive or exclusive, but rather identify the most important concerns for developing embedded analysis components. 
%Moreover this material provides arguments for deciding if a concern is fundamental to the conceptual model and should be captured by the interface, or is better left at the discretion of the implementation.

\subsection{It starts with data}

The role and shape of \emph{data} is one of the main characteristics that distinguishes scientific computing. In most general purpose programming languages, \emph{data structures} are instances of classes with well defined fields and methods. Similarly, databases typically use schemas or table definitions to enforce the structure of data on disk. This ensures that a table returned by a given \SQL query always contains exactly the same structure with the requested fields; the only varying property between several executions of a query is the number of returned rows. Also the time needed for the database to process the request usually depends only on the amount of records in the database.
Strickly defined structures make it possible to write code implementing all required operations in advance without knowing the actual \emph{content} of the data. It also creates a clear seperation between developers and users. Most applications do not give users direct access to raw data. Developers focus in implementing code and designing data structures, whereas users merely get to execute a limited set of operations.

This paradigm does not work for scientific computing. Developers of statistical software have relatively little control over the structure, content and quality of the data. Data analysis usually starts with the user supplying a dataset, which is rarely pretty. Real world data come in all shapes and formats. They are messy, have inconsistent structures and invisible numeric properties. Therefore statistical programming languages define data structures relatively loosely and instead implement a rich lexicon for interactively inspecting, manipulating, restructuring, subsetting, combining, playing and computing on data. Unlike software operating on well defined data structures, it is nearly impossible to write code that accounts for any scenario and will work for every possible dataset. Many functions are not applicable to every instance of a particular class, or might behave differently based on dynamic properties of the data, such as size or dimensionality. For these reasons there is also less clear of a seperation between developers and users in scientific computing. The data analysis process involves simultaneously debugging of code and data where the user iterates back and forth between manipulating and analyzing the data. Implementations of statistical methods tend to be very flexible with many parameters and settings to specify behavior for the broad range of possible data. And still the user might have to go through many steps of cleaning and reshaping to give data the appropriate structure and properties to perform a particular analysis. 

Informal operations and loosely defined data structures are typical characteristics of scientific computing. They give a lot of freedom to implement powerful and flexible tools for data analysis. However they also complicate interfacing of statistical methods. In the context of an interactive \UI, the user iterates from one step to the next using a trial and error strategy. However embedded systems require a degree of type-safety, predictability and consistency to facilitate reliable \texttt{I/O} between components. These features are native to databases or many object orriented languages, but require substantial effort for statistical software. 


\subsection{Functional programming}

Many different programming languages and styles exists, each with their own strengths and limitations. Scientific computing languages typically use a functional style of programming, where methods take a role and notation similar to \emph{functions} in mathematics. This has obvious benefits for numerical computing. Statistical methods are written as $y = f(g(x))$ (rather than $y = x.g().f()$ notation) and a functional syntax results in intuitive code for implementing algorithms. 

However most popular general purpose languages take a more imperative and object oriented approach. In many ways, object-oriented programming can be considered the opposite of functional programming \citep{pythonfunctional}. Here methods are invoked on an object and modify the \emph{state} of this particular object. Object-oriented languages typically implement inheritance of fields and methods based on object classes or prototypes. Many software engineers prefer this style of programming because it is more powerful to handle complex data structures. The success of object oriented languages has also influenced scientific computing, resulting in multi-paradigm systems. For example, languages such as \Julia and \R use multiple dispatch to dynamically assign function calls to a particular function based on the type of arguments. This brings certain object oriented benefits to functional languages, but also complicates scoping and inheritance. 

A comparitive review on programming styles is beyond the scope of this research. But what is relevant to us is how conflicting paradigms might complicate interfacing analysis components. For example, in the context of web services, the \emph{Representational State Transfer} style (for short: \REST) described by \cite{fielding2000architectural} is very popular among web developers. A \emph{restful} \API maps every \URL to a \emph{resource} and \HTTP requests are used to modify the \emph{state} of a resource, which results in a simple and elegant \API. Unfortunately, \REST does not map very naturally to the functional paradigm of statistical software. Languages where functions are first class citizens suggest more \RPC flavored interfaces, which according to Fielding are by definition not restful \citep{fielding2008rest}. This does not mean that such a component is incompatible with other pieces. As long as components honor the rules of the protocol (i.e. \HTTP) they will work together. However conflicting programming styles can be a source of friction for embedded scientific computing. Strongly object-oriented frameworks or developers might require some additional effort to get comfortable with components implementing a more functional paradigm.



\subsection{Graphics}

Another somewhat domain specific feature of scientific computing is native support for graphics. Most statistical software packages include programs to draw plots and charts in some form or another. In contrast to data and functions which are language objects, the graphics device is implemented as a separate output stream. As such, drawings on the canvas are a side effect rather than a return value of function calls. This works a bit similar to manipulating document object model (\texttt{DOM}) elements in a browser using \JavaScript. In most interactive statistical software, graphics appear to the user in a seperate window. The state of this graphics device can not easily be stored or serialized as is the case for functions and objects. Products typically include functionality to export an \emph{image} of the graphics device to a file using \texttt{png}, \texttt{svg} or \texttt{pdf} format. However this image is merely a snapshot; it does not contain the actual state of the device can not be reloaded for further manipulation.

First class citizenship of graphics is an important part of interfacing scientific computing. However, output containing both data and graphics makes the design of a general purpose \API more difficult. The system needs to capture the return value as well as graphical side effects of a remote function call. Furthermore the interface should allow for generating graphics without imposing restrictions on the format or formatting parameters. Users often want to use a simple bitmap format such as \texttt{png} for previewing a graphic, but have the option to export the same graphic to a high quality vector based format such as \texttt{pdf} for publication. Because statistical computation is often expensive and non-deterministic, we can not simply reconstruct the graphic from scratch only to retrieve it in another format. Hence the \API needs to incorporate the notion of a graphics device in a way independent of the imaging format. 

\subsection{Numeric properties and missing values}

We already mentioned how loosely defined data structures in scientific computing can impede type safety of data \texttt{I/O} in analysis components. However statistical methods can choke on the actual content of data as well. Sometimes problematic data can easily be spotted, but often it is nearly impossible to detect these ahead of time. Applying statistical procedures to these data will then result in errors, even though the code and structure of the data are perfectly fine. For example, many statistical models build on matrix decompositions that require the data to follow certain numeric properties. The \emph{rank} of a matrix is one such property which measures the nondegenerateness of the system of linear equations. When a matrix $A$ is rank deficient, the equation $Ax=b$ does not have a solution when $b$ does not lie in the range of $A$. Attempting to solve this equation results in similar computational problems as division by zero. Accounting for this ahead of time is nearly impossible because numeric properties are invisible until they are actually calculated. But perhaps just as difficult is explaining the user or software engineer that these errors are not a bug, and can not be fixed. The procedure just does not work for this particular dataset.

Another example of problematic data is presented by \emph{missing} values. Missingness in statistics means that the value of a field is unknown. However, missing data should not be confused with no data or \texttt{null}. Missing values are often \emph{non ignorable}, meaning that the missingness itself is information that needs to be accounted for in the modeling. A standard textbook example is when we perform a survey asking people about their salary. Because some people might refuse to provide this information, the data contains missing values. However, the missingness is probably \emph{not completely at random}: respondents with high salaries might be more reluctant to provide this information than respondents with a median salary. If we would calculate the mean salary from our data ignoring the missing values, the estimate is likely biased. To obtain a more accurate estimate of the average salary, missing values need to be incorporated in the estimation using a more sophisticated model. 

Statistical programming languages can define serveral types of missing or non-finite values such as \texttt{NA}, \texttt{NaN} or \texttt{Inf}. These are typically implemented as special primitives, which is one of the benefits of using a \DSL. Statistical functions and methods have built-in procedures and options to specify how to handle missing values encountered in the data. However the notion of missingness is foreign to most languages and software outside of scientific computing. They are a typical domain-specific phenomenon that can cause technical problems in data exchange with other systems. And like numeric properties, the concept of values containing no actual value is likely to cause confusion among developers or users with limited experience in data analysis. However failure to properly incorporate missing values in the data can easily lead to errors or incorrect results, as the example above illustrated.

\subsection{Non deterministic and unpredictable behavior}

Most software applications are expected to produce consistent output in a timely manner, unless something is very wrong. However this does not generally hold for scientific computing. The previous section explained how problematic data can cause exceptions or unexpected results. But many analysis methods are actually non-deterministic or unpredictable by nature.

Statistical algorithms often repeat some calculation until a particular convergence criterion is reached. Starting values and minor fluctions in the data can have showball effect on the course of the algoritm. Therefore several runs can result in wildly varying outcomes and execution times. Moreover, convergence is often not guaranteed: unfortunate input can get a process stuck in a local minimum or send it off into the wrong direction. It is often difficult predict and control such scenarios and edge cases a-priori in the implementation. Monte Carlo techniques are even less predictable because they are specifically designed to behave randomly. For example, \MCMC methods use a Markov-Chain to simulate random walk through a (high-dimensional) space such as a multivariate propbability density. These methods are a powerful tool for Bayesian analysis, simulation studies and numerical integration. In \MCMC, each execution of the random walk will yield different outcomes, but under general conditions the process will converge to the value of interest. However due to randomness it is possible that some of the runs or chains get stuck and need to be terminated or disregarded.

Unpredictability of statistical methods underlies many technical problems for embedded scientific computing that are not present in for example a database. This can sometimes surprise software engineers expecting deterministic behavior. Methods in scientific computing are rarely absolutely guaranteed to be successful for arbitrary data. Assuming that a procedure will always return timely and consistently because it did so with testing data is very dangerous. Within an interactive session the user can simply interrupt a process that seems to have gotten stuck. However for embedded modules, unpredictability needs to be accounted for in the design of the system. At a very minimum, the system should be able to detect and terminate processes that have not completed when some timeout is reached. But preferably we need a layer or meta functionality to control and monitor executions, either manually or automatically. 

\subsection{Managing experimental software}

In scientific computing we are often dealing with inventive, volatile and experimental  software. This is a big cultural difference with many general purpose languages such as \texttt{python}, \texttt{Java}, \texttt{C++} or \texttt{JavaScript}. The latter communities include professional organizations and engineers committed to implementing and maintaining production quality libraries. However most authors of open source statistical software do not have the expertise and resources to meet such standards. Contributed code in languages such as \texttt{R} was often written to accompany a scientific article proposing novell ideas, models, algorithms or programming techniques. The script or package serves as an illustration of the presented ideas and was implemented by academics or students. The quality of such contributions varies a lot, no active support or maintenance should be expected from the authors. Furthermore, package updates can sometimes introduce radical changes based on new insights. Therefore, contributed code is often merely a starting point for analysis and needs to be tweaked and tailored to fit a particular problem or dataset.

Because traditional data analysis does not really have a notion of production, this has never been a major problem. The emphasis in statistical software has always been on innovation rather than continuity. Experimental code is usually good enough for interactive data analysis where it suffices to manually make a script or package work for the dataset at hand.
Authors of statistical software often assume that the user will spend some effort to manage dependencies and debug the code. However integrated components require a greater degree of reliability and continuity which introduces a source of technical and cultural friction for embedded scientific computing. 
%This will likely not change overnight. Proposals to make statistical software play nicer with other software have been received with little understanding and sympathy within the community. 
This makes the ability to manage unstable software, facilitate rapid change, sandbox modules and manage failure important concerns of embedded scientific computing.

\subsection{Interactivity and error handling}

In general purpose languages, run-time errors are typically caused by a bug or some sort of system failure. Exceptions are only raised when the software can not recover and usually result in termination of the process. Error messages contain information such as calling stacks to help the programmer discover where in the code a problem occured. Software engineers go through great trouble to prevent potential problems ahead of time using smart compilers, unit tests, automatic code analysis and continuous intergration. Errors that do arise during production are usually not displayed to the user, but rather the administrator is notified that the system urgently needs attention. The user gets to see an apology at best.

In scientific computing, errors play a very different role. As a consequence of some of the characteristics discussed earlier, interactive debugging is a natural part of the user experience. Errors in statistics do not necessarily indicate a bug in the software, but rather a problem with the data or some interaction of the code and data. The statistician goes back and forth between cleaning, manipulating, modeling, vizualizing and interpreting to study patterns and relations in the data. This interactive debugging of data and code comes down to a lot of trial and error. Problems with missing values, outliers, types, structures, degrees of freedom or numeric properties might not reveal themselves until we try to fit a model or create a plot. Often times, exceptions raised by a statistical methods are a sign that data needs additional work. This makes error messages an important source of information for the statistician to get to know a dataset and its difficulties. And while debugging the data we might discover limitations of the analysis methods. In practice we often find out that a particular dataset requires us to research or implement additional techniques because the standard tools do not suffice or are inappropriate.

Interactive error handling is one of the reasons that there is no clear distinction between development and production in scientific computing. When interfacing with analysis modules it is important that the role of errors is understood and recognised. An \API must be able to handle exceptions and report error messages to the user, and certainly not crash the system. The role of errors and interactive debugging in data analysis is sometimes confusing to developers outside of our community. Some popular commercial products seem to have propagated the belief that data analysis comes down to applying a magical formula to a dataset, and no intelligent action is required on the side of the user. However systems that only support such canned analyses don't do justice to the wide range of methods that statistics has to offer. In practice, interactive data debugging is a crucial part of data analysis and embedded scientific computing.

\subsection{Security and resource control}

Somewhat related to the above are special needs in terms of security. Most statistical software currently available is primarily designed for interactive use on the local machine. Therefore access control is not considered an issue and the execution environment is entirely unrestricted. However embedded modules or public services require implementation of security policies to prevent malicious or excessive use of resources. This in itself is not a unique problem. Most scripting languages such as \texttt{php} or \texttt{python} do not enforce any access control and assume security will be implemented on the application level. However there are two domain specific aspects that complicate the problem. 

The first issue is that statistical software can be demanding and greedy with hardware resources. Numerical methods are expensive both in terms of memory and \CPU. Fair use policies are not really feasible because excessive use of resources often happens unintentionally. For example, an overly complex model specification or algorithm getting stuck might end up consuming all available memory and \CPU until manually terminated. When this happens on the local machine, the user can easily interrupt the process prematurely by sending a \texttt{SIGINT}, i.e. pressing \texttt{CTRL+C} or \texttt{ESC}. However in a shared environment this needs to be regulated by the system. Embedded scientific computing requires technology and policies that can manage and limit memory allocation, cycles, disk space, concurrent processes, network traffic, etc. The degree of flexibility offered by implementation of resource management is an important factor in the scalability of a system. Fine grained control over system resources consumed by individual tasks allows for serving many concurrent users without sacrificing reliabilty. 

The second domain specific security issue is caused by the need for arbitrary code execution. A traditional application security model is based on user role privileges. For example in a standard web application, only a developer or administrator can implement and deploy actual code. The application merely exposes predefined functionality; users are not allowed to execute arbitrary code on the server. Any possibility of code injection is considered a security vulnerability and when found the server is potientially compromised. However as already mentioned, lack of segregation between users and developers in statistics gives limited use to applications that restrict users to predefined scripts and canned services. To support actual data analysis, the user needs access to the full language lexicon to freely explore and manipulate the data. The need for arbitrary code execution disqualifies user role based privileges and demands a more sophisticated security model. 
%Therefore the security policies are for the most part a concern of the server implementation and do not need to be described in the interface.


\subsection{Reproducible research}

Replication of findings is one of the main principles of the scientific method. In quantitative research, a necessary condition for replication is reproducibiltity of results. The goal of reproducible research is to tie specific instructions to data analysis and experimental data so that scholarship can be recreated, better understood and verified \citep{cranRR}. Even though the ideas of replication are as old as science itself, reproducibility in scientific computing is still in its infancy. Tools are available that assist users in documenting their actions, but to most researchers these are not a natural part of the daily workflow. However the importance of replication in data analysis is increangly recognised and support for reproducibility is becoming more influential in the design of statistical software.

Reproducibility changes what constitutes the main product of data analysis. Rather than solely output and conclusions, we are interested recording and publishing the entire \emph{analysis process}. This includes all data, code and results but also external software that was used arrive at the results. Reproducibility puts high requirements on software versioning. More than in other fields it is crucial that we diligently archive and administer the precise versions or branches of all scripts, packages, libraries, plugins that were somehow involved in the process. If an analysis involves randomness, it is also important that we keep track of which seeds and random number generators were used. In the current design of statistical software, reproducibility was mostly an afterthought and has te be taken care of manually. In practice it is tedious and error-prone. There is a lot of room for improvement through software that incorporates reproducible practices as a natural part of the data analysis process.

Whereas reproducibility in statistics is acknowledged from a transparency and accountability point of view, it has enourmous potential to become much more than that. There are obvious parallels between reproducible research and revision control in source code management systems. Technology for automatic reproducible data analysis could revolutionize scientific collaboration, similar to what \texttt{git} has done for software development. A system that keeps track of each step in the  analysis process like a \emph{commit} in software versioning would make peer review or follow-up analysis more practical and enjoyable. 
%A scientific publication would no longer be considered an end product, but rather the starting point of scientific debate. 
When colleagues or reviewers can easily reproduce results, test alternative hypotheses or recycle data, we achieve greater trustworthiness but also multiply return on investment of our work. Finally an open kitchen can help facilitate more natural ways of learning and teaching statistics. Rather than relying on general purpose textbooks with artificial examples, scholars directly study the practices of prominent researchers to understand how methods are applied in the context of data and problems as they appear specifically in their area of interest.