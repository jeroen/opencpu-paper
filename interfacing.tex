%meta
\documentclass{article}
\author{Jeroen Ooms}
\title{The OpenCPU System:\\ Towards a Universal Interface for Scientific Computing}

%some packages
\usepackage{url}
\usepackage{fullpage}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage[round]{natbib}
\usepackage{fancyvrb}
\usepackage[toc,page]{appendix}
\usepackage{breakurl}

%support for accents
\usepackage[utf8]{inputenc}

%use vspace instead of indentation for paragraphs
\usepackage{parskip}

%extra line spacing
\usepackage{setspace}
\setstretch{1.2}

%custom commands
\newcommand{\SQL}{\texttt{SQL}\xspace}
\newcommand{\CGI}{\texttt{CGI}\xspace}
\newcommand{\R}{\texttt{R}\xspace}
\newcommand{\HTTP}{\texttt{HTTP}\xspace}
\newcommand{\TCP}{\texttt{TCP}\xspace}
\newcommand{\IP}{\texttt{IP}\xspace}
\newcommand{\GUI}{\texttt{GUI}\xspace}
\newcommand{\CLI}{\texttt{CLI}\xspace}
\newcommand{\UI}{\texttt{UI}\xspace}
\newcommand{\API}{\texttt{API}\xspace}
\newcommand{\RPC}{\texttt{RPC}\xspace}
\newcommand{\ID}{\texttt{ID}\xspace}
\newcommand{\OpenCPU}{\texttt{OpenCPU}\xspace}

%actual doc
\begin{document}

\maketitle

\begin{abstract}
A vital part of facilitating embedded scientific computing is specification of the interface. Clients or systems integrating analytical components require a simple well interface that exposes desired functionality and hides implementation details. Currently, most statistical software is designed for UI rather than API interaction which presents some challenges when turning methods into components. Because scientific computing is highly interactive, the role of \emph{state} is the central problem in the design of an interface. This paper introduces OpenCPU: a system for embedded scientific computing based on R. The OpenCPU API is described in detail and used to exemplify interoperability of statistical methods by decoupling application from statistical computation. Because OpenCPU leverages native features of HTTP, it can easily be extended and no knowledge of R is required to implement clients. We finish by showing how the OpenCPU makes it trivial to turn R packages into webapps.
\end{abstract}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\vspace{8pt}

\begin{quote}
\emph{Premature optimization is the root of all evil.} --- Donald Knuth
\end{quote}

\section{Introduction}

- what is embedded scientific computing
- why is it increasingly important
- scale


%Methods for scientific computing are typically implemented in specialized packages assisting the statistician in all facets of the data analysis process. Some products are optimized for use via a command line interface (\CLI) whereas others focus on the graphical user interface (\GUI), yet most emphasize the user interface (\UI) as primary point of interaction. However the changing role of data analysis in modern society is demanding better interoperability of statistical software. Embedding statistical methods in the next generation of software systems requires better application programming interaces (\API). This means moving away from manual solutions towards component-based analysis units. This research is a piece in a series of contributions advocating for a more flexible and integrated approach to data analysis. Throughout this paper we refer to this under the umbralla term of \emph{embedded scientific computing}.

%A vital part of facilitating embedded scientific computing is specification of the interface. Clients or systems integrating analytical components require a well defined \API that exposes desired functionality while abstracting language specifics and implementation details. Defining an interoperable entry point allows servers to expose computational methods, and clients to call these methods. Such a universal interface separates the domain logic of scientific computing from application logic and lower-level logistics. 

%An interesting analogy is provided by database interfaces. For example, the \texttt{SQL} language provides an interoperable interface for managing data held in a relational database management system (\texttt{RDBMS}). \texttt{SQL} consists of a data definition language and a data manipulation language that separates client and server responsibilities. The standard interface allows for implementing clients and applications building on relational data without any database implementation knowledge. Many \texttt{SQL} servers such as \texttt{MySQL} and \texttt{MS SQL Server} implement this standard for various platforms and languages and providing different optimization and configuration options. Naturally, some \texttt{SQL} extensions and dialects exist. But the core of the language is widely implemented and has greatly contributed to the wide adoption of relational databases.

\subsection{The need for abstraction}

Previous work in embedded scientific computing has mostly been limited to low-level tools for connecting statistical software to general purpose environments. As a result, ``bridges'' are available to execute an \R script or process from inside most popular languages. For example, \texttt{JRI} \citep{rjava}, \texttt{RInside} \citep{eddelbuettel2011rcpp}, \texttt{rpy2} \citep{gautier2008rpy2} or \texttt{RinRuby} \citep{dahl2008rinruby} can be used to call \R from respectively \texttt{Java}, \texttt{C++}, \texttt{Python} or \texttt{Ruby}. \cite{heiberger2009r} provide a set of tools to execute \R code from \texttt{DCOM} clients on Windows, mostly to support running \R in Microsoft Excel. The Apache2 module \texttt{rApache} (\texttt{mod\_R}) makes it possible to run R scripts from the Apache2 web server \citep{horner2013rapache}. The \texttt{littler} program provides hash-bang capability for \R, as well as simple command-line and piping use on \texttt{unix} \citep{littler}. Finally, \texttt{Rserve} is \texttt{TCP/IP} server which provides low level access to an R session over a socket \citep{urbanek2013rserve}. 

However even though these tools have been available for several years, they have not been able to facilitate the big break through of \R as a ubiquitous statistical engine. Given the enourmous demand for analysis and visualization these days, the adoption of \R for embedded scientific computing is actually quite underwhelming. In my experience, the primary cause for the limited success is that low-level bridges are difficult to implement, do not scale very well, and leave the most challenging problems unsolved. Substantial plumbing and expertise of \R internals is required for building actual applications on these tools. Clients are supposed to generate and push \R syntax, make sense of \R's internal \texttt{C} structures and write their own framework for managing jobs, graphics, security, data interchange, etc. In practice this is very difficult, results in messy code, and often leads to unstable applications when important issues were not fully taken care of. The tools are also impractical from a human point of view. Building a web application with for example \texttt{JRI}, \texttt{RInside} or \texttt{Rserve} requires a web developer that is also an expert in \R, as well as \texttt{C++} or \texttt{Java}. Because \R is such a domain specific language, this combination of skills is very rare. 

What is needed to scale up embedded scientific computing is a system that decouples the application layer from the computational back-end, similar to how e.g. \texttt{SQL} separates the application and database. A well defined separation of concerns allows for independent development of client and server components, by different people with different background and expertise. Idealy, the system interface should describe domain logic of scientific computing and abstract implementation details as much as possible. This gives clients a universal point of interaction to perform data operations without any specialized knowledge about the computational language or infrastructure. At the same time, statisticians can expose analysis methods for use in applications without having to learn web development or other tools. Careful isolation of components along the lines of domain logic allows developers to focus on their expertise using their tools of choice. This vision forms the starting point of this research. 




\subsection{The \OpenCPU system}

The \OpenCPU system is used as a reference implementation throughout this paper. \OpenCPU defines an \HTTP \API that builds on \emph{The R Project for Statistical Computing} \citep{R}, for short: \R. The \R language is the obvious candidate for a first implementation of this kind. It is currently the most popular open source software for data analysis and considered by many statisticians as the \emph{de facto} standard of data analysis. The huge \R community provides both the tools and use-cases needed to develop and experiment with this new approach to scientific computing. Currently only \R has the required scale and foundations to really put our ideas to the test. For this reason the paper discusses concerns of scientific computing mostly in the context and terminology of \R. We do try to capture the general logic of data analysis rather than that of a particular language, however in practice it is very difficult to study and discuss software without an implementation. Therefore we rely on \R to illustrate and exemplify the concepts of scientific computing. 

For example, the conventional container format for namespacing a collection of data and functions in \R is a \emph{package}. Other products might use different mechanics and jargon for combining and publishing a set of objects. However any system needs some notion of namespaces and dependencies analogous to packages in \R. For sake of simplicity this paper discusses such concepts mostly from the way they take shape in \R. But we want to stress that even though our research and the \OpenCPU system are inspired by and tailored to the way things work in \R, the approach should generalize quite naturally to other computational back-ends. The purpose of the \OpenCPU system is not merely to provide software, but also not to propose an \API standard. \OpenCPU illustrates what an abstracted interface to scientific computing could looks like. The implementation allows us and others to put this new approach into practice and get first hand experience with the problems and opportunities in this unexplored field. 


\subsection{Scope of this research}

- component based software engineering: interface.
- goal: explore this new space
- identify challenges and opportunities
- to do so: first explore domain logic
- opencpu: example throughout

The remainder of this paper is structured as follows. Section 2 gives a general introduction to practices and domain logic of scientific computing. The purpose, problems and practices of this unique field are described to set the stage for our system and interface. Section 3 then continues with a discussion of some previous work and the state problem, which is the most central challenge in the design of an \API for embedded analysis. Section 4 then introduces the \OpenCPU software. We first explain the history and philosophy of the system and slowly work our way to a more detailed description of the \API. We end with  examples demonstrating some existing and potential applications.


\section{Practices and domain logic of scientific computing}

This section gives a helicopter view of some of the practices and domain logic of scientific computing. The field and software of data analysis is unlike any other domain. Statistical computing has intrinsic properties and peculiarities that present difficulties when embedding statistical methods into systems. It is my impression that lack of understanding both within and outside the statistics community about what distinguishes scientific computing from other branches of computer science underlies many of the technical problems. At the same time there are enourmous opportunities to facilitate better science and data analysis using modern technogy that might be overlooked due to a disconnect between statisticians and software engineers. The purpose of this section is to provide a high-level introduction to problems and practices of scientific computing.

The topics in this section are presented in a somewhat informal and subjective manner, and are not intended to be exhaustive or exclusive. We merely scratch the surface by touching on some relevant themes in the context of this resarch. The reader should get a sense of what makes data analysis unique, and why the software landscape is dominated by Domain Specific Languages (DSL). Along the way we highlight some of the major difficulties and pitfalls, which sets the stage for working towards an interface definition in subsequent sections.

\subsection{It starts with data}

The role and shape of \emph{data} is one of the main characteristics that distinguishes scientific computing. In most general purpose programming languages, \emph{data structures} are instances of classes with predefined properties and methods. Data on disk is stored in a database using schemas to enforce structure. This makes inserting and retrieving data reliable and predicable: a table returned by a given \SQL query always contains exactly the same structure with the requested fields. The only varying property between several executions of a query is the number of returned rows. Also the time needed for the database to process the request usually depends only on the amount of records in the database. This provides the foundations to write code that implements all required operations in advance without knowing the content of the data. It also allows for a pretty sharp seperation between developers and users. Most applications do not give users direct access to raw data structures. Developers focus in implementing code and designing data structures, whereas users merely execute code.

This paradigm does not work for scientific computing. Developers of statistical packages have relatively little control over the structure, content and quality of the data. Data analysis usually starts with the user supplying a dataset, which is rarely pretty. Real world data come in all shapes, forms and formats. They are messy, have inconsistent structures, dynamic attributes and invisible numeric properties. Therefore statistical programming languages define data structures relatively loosely, and instead implement a rich lexicon for interactively inspecting, manipulating, restructuring, subsetting, combining, playing and computing on data. Unlike software operating on well defined data structures, it is nearly impossible to write code that accounts for any scenario and will work for every possible dataset. Many functions and methods are not applicable to every instance of a particular class, or might behave differently based on dynamic properties of the data, such as size, dimensionality or rank. For these reasons there is also less clear of a seperation between developers and users in scientific computing. The data analysis process involves simultaneously debugging of code and data where the user iterates back and forth between manipulating the data and modeling/analysis/visualization. Implementations of statistical methods tend to be very flexible with many parameters and settings to specify and customize behavior for the broad range of possible datasets. And still the user might have to go through many steps of cleaning and reshaping to give data the appropriate structure and properties to perform a particular analysis. 

Informal operations and loosely defined data structures are typical characteristics of scientific computing. They give a lot of freedom to implement powerful and flexible operations for data analysis. However they also complicate interfacing of statistical methods. In the context of an interactive \UI, the user iterates from one step to the next using a trial and error strategy. However embedded systems require a degree of type-safety, predictability and consistency to facilitate reliable I/O between components. These features are native to databases or many object orriented languages, but require substantive effort for statistical software. 


\subsection{Functional programming}

Many different programming languages and styles exists, each with their own strengths and weaknesses. Scientific computing languages typically use a functional style of programming, where methods take a role and notation similar to \emph{functions} in mathematics. This has obvious benefits for numerical computing. Statistical methods are written as $y = f(g(x))$ (rather than $y = x.g().f()$ notation), and a functional syntax results in intuitive code for implementing statistical algorithms. 

However most popular general purpose languages take a more imperative and object oriented approach. In many ways, object-oriented programming can be considered the opposite of functional programming \citep{pythonfunctional}. Here methods are invoked on an object and modify the \emph{state} of this particular object. Object-oriented languages typically implement inheritance of fields and methods based on object classes or prototypes. Software engineers tend to prefer this style of programming because it is more powerful to handle complex data structures. The success of object oriented languages has also influenced scientific computing, resulting in multi-paradigm systems. For example, languages such as Julia and R use multiple dispatch to dynamically assign function calls to a particular function based type of arguments. This brings certain object oriented benefits to functional languages, but also complicates scoping and inheritance. 

A comparitive review on programming styles is beyond the scope of this research. But what is relevant to us is how conflicting paradigms might complicate interfacing analysis components. For example, in the context of web services, the \emph{Representational State Transfer} (REST) style described by \cite{fielding2000architectural} is very popular among web developers. A restful API maps URLs to \emph{resources} and http requests are used to modify the \emph{state} of such a resource. The REST style results in simple and elegant HTTP interfaces for object orriented systems. Unfortunately, REST does not map very naturally to the functional paradigm of statistical software. Languages where functions are first class citizens suggest more RPC flavored interfaces, which according to Fielding are by definition not restful. This does not mean that such a component is incompatible with other pieces. As long as components honor the rules of the protocol (i.e. HTTP) they will work together. However conflicting programming styles can be a source of friction for embedded scientific computing. Strongly object-oriented frameworks or developers might require some additional effort to get comfortable with components implementing a more functional paradigm.



\subsection{Graphics}

Another somewhat domain specific feature of scientific computing is native support for graphics. Most statistical software packages include programs to draw plots and charts in some form or another. In contrast to data and functions which are language objects, the graphics device is implemented as a separate output stream. As such, drawings on the canvas are a \emph{side effect} rather than a return value of function calls. This works a bit similar to manipulating DOM elements in a browser using JavaScript. In most interactive statistical software, graphics appear to the user in a seperate window. The state of this graphics device can not easily be stored or serialized as is the case for functions and objects. Most software packages do include functionality to export an \emph{image} of the graphics device to a file using \texttt{png}, \texttt{svg} or \texttt{pdf} format. However this image is merely a snapshot; it does not contain the actual state of the device can not be reloaded for further manipulation.

First class citizenship of graphics is an important part of interfacing scientific computing. However, output containing both data and graphics makes the design of a general purpose API more difficult. A remote function call needs to capture both return value as well as graphical side effects. Furthermore the interface should allow clients to generate graphics without imposing restrictions on the format or format parameters. Users often want to use a simple bitmap format such as \texttt{png} for previewing a graphic, but have the option to export the same graphic to a high quality format such as \texttt{pdf} for publication. And because statistical computation is often expensive and non-deterministic, we can not simply reconstruct a plot from scratch only to retrieve it in another format. Hence a natural API needs to incorporate the notion of a graphics device in a way independent of the imaging format. 

\subsection{Numeric properties and missing values}

We already mentioned how loosely defined data structures in scientific computing can impede type safety of data I/O in analysis components. However statistical methods can choke on the actual content of data as well. Sometimes problematic data can easily be spotted, but often it is nearly impossible to detect these ahead of time. Applying statistical procedures to these data will then result in errors, even though the code and structure of the data are perfectly fine. For example, many statistical models build on matrix decompositions that require the data to follow certain numeric properties. The \emph{rank} of a matrix is one such property which measures the nondegenerateness of the system of linear equations. When a matrix $A$ is rank deficient, the equation $Ax=b$ does not have a solution when $b$ does not lie in the range of $A$. Attempting to solve this equation results in similar computational problems as division by zero. Accounting for this ahead of time is nearly impossible because numeric properties are invisible until they are actually calculated. But perhaps just as difficult is explaining user or software engineer that these errors are not a bug, and can not be fixed. The procedure just does not work for this particular dataset.

Another example of problematic data is presented by \emph{missing} values. Missingness in statistics means that the value of a field is unknown. However, missing data should not be confused with no data or \texttt{null}. Missing values are often \emph{non ignorable}, meaning that the missingness in itself is information that needs to be accounted for in the modeling. A standard textbook example is when we perform a survey asking people about their salary. Because some people might refuse to provide this information, the data contains missing values. However, the missingness is probably not ``completely at random": people with high salaries tend to be more reluctant to provide this information than people with a median salary. If we would calculate the mean salary from our data ignoring the missing values, the estimate is likely biased. To obtain a more accurate estimate of the average salary, missing values need to be incorporated in the estimation using a more sophisticated model. 

Statistical programming languages can define serveral types of missing or non-finite values such as \texttt{NA}, \texttt{NaN} or \texttt{Inf}. These are implemented as special primitives, which is one of the benefits of a DSL. Statistical functions and methods have special procedures and options to specify how to handle missing values encountered in the data. However the notion of missingness in foreign to most languages and software outside of scientific computing. They are a typical domain-specific phenomenon that can cause technical problems in data exchange with other systems. And like numeric properties, the concept of a values containing no value is likely to cause confusion among developers or users with limited experience in data analysis. However failure to properly incorporate missing values in the data can easily lead to errors or incorrect results, as the example above illustrated.

\subsection{Non deterministic and unpredictable behavior}

Most software applications are expected to produce consistent output in a timely manner, unless something is very wrong. However this does not generally hold for scientific computing. The previous section explained how problematic data can cause exceptions or unexpected results. But many analysis methods are actually non-deterministic or unpredictable by nature.

Statistical algorithms often repeat some calculation until a particular convergence criterion is reached. Starting values and minor fluctions in the data can have showball effect on the course of the algoritm. Therefore several runs can result in wildly varying outcomes and execution times. Moreover, convergence is often not guaranteed: unfortunate input can get a process stuck in a local minimum or send it off into the wrong direction. It is often difficult predict and control such scenarios and edge cases a-priori in the implementation. Monte-carlo techniques are even more unpredictable, because they are specifically designed to behave randomly. For example, MCMC methods use a Markov-Chain to simulate random-walk through a (high-dimensional) space such as a multivariate propbability density. MCMC methods are popular in simulation studies or numeric integration. Each execution of the random walk will yield different outcomes, but eventually things should converge to the value of interest. However due to randomness it is possible that some of the runs or chains get stuck and need to be terminated or disregarded.

The unpredictability aspect severely complicates embedding of statistical software, and can sometimes surprise software engineers assuming deterministic behavior. Methods in scientific computing are rarely 100\% guaranteed to be successful for arbitrary data. Assuming that a procedure will always return timely and consistently because it did so with a test dataset is very dangerous. Within an interactive session the user can simply interrupt a process that seems to have gotten stuck. However for embedded modules, unpredictability needs to be accounted for in the design of the system. At a very minimum, the system should be able to detect and terminate processes that have not completed when some timeout is reached. But preferably we need a layer or meta functionality to control and monitor jobs and executions, either manually or automatically.  This aspect of scientific computing introduces major complications in comparison with embedding types of software that behave more predictably such as a database. 

\subsection{Experimental software}

In scientific computing is we often need to deal with inventive, volatile and experimental  software. This is a big cultural difference with many general purpose languages such as \texttt{python}, \texttt{Java}, \texttt{C++} or \texttt{JavaScript}. The latter communities include professional organizations and engineers committed to implementing and maintaining ``production quality'' libraries. However most authors of open source statistical software do not have the expertise and resources to meet such standards. Contributed code in languages such as \texttt{R} was often written to accompany a scientific article proposing novell ideas, models, algorithms, or programming techniques. The script or package serves as an illustration or proof of concept of the presented ideas and was implemented by academics or students. The quality of such contributions varies a lot and no active support, maintenance or backward compatibility should be expected from the authors. Sometimes an update to a package radically changes things based on new ideas or insights. Contributed code might need to be tweaked, customized and tailored to fit a particular problem or dataset. In some cases, published code merely serves as an example of a particular technique, and only provides a starting point for an analysis. 

Because traditional data analysis does not really have a notion of production this is no problem. The emphasis is on innovation rather than continuity. Unstable software is good enough for interactive data analysis where it suffices to manually make a script or package work for the dataset at hand. However integrated components require a greater degree of reliability and continuity. This introduces a source of technical and cultural friction for embedded scientific computing. Statistical software is traditionally at the top top of the food chain. It is assumed that the user will manually manage dependencies and debugging code to make things work. This will likely not change overnight. Proposals to make statistical software play nicer with other software have been received with little understanding and sympathy within the community. Therefore a system should be able to manage unstable software, facilitate rapid change, sandbox modules and manage failure.

\subsection{Interactivity and error handling}

In general purpose languages, run-time errors are typically caused by a bug or some sort of system failure. Exceptions are only raised when the software can not recover and usually result in termination of the process. Error messages contain information such as calling stacks to help the programmer discover where in the code a problem occured. Software engineers go through great trouble to prevent potential problems ahead of time using smart compilers, unit tests, automatic code analysis and continuous intergration. Errors that do arise during production are usually not displayed to the user, but rather the administrator is notified that the system urgently needs attention. The user gets to see an apology at best.

In scientific computing, errors play a very different role. As a consequence of some of the characteristics discussed earlier, interactive debugging is a natural part of the user experience. Errors in statistics do not necessarily indicate a bug in the software, but rather a problem with the data or some interaction of the code and data. The statistician goes back and forth between cleaning, manipulating, modeling, vizualizing and interpreting to study patterns and relations in the data. This interactive debugging of data and code comes down to a lot of trial and error. Problems with missing values, outliers, types, structures, degrees of freedom or numeric properties might not reveal themselves until we try to fit a model or create a plot. Often times, exceptions raised by a statistical methods are a sign that data needs additional parsing, munging or cleaning. This makes error messages an important source of information for the statistician to get to know a dataset and its difficulties. And while debugging we discover the strenghts and limitations of the analysis methods. In practice we often find out that a particular dataset requires us to research or implement additional techniques because the standard tools do not suffice or are inappropriate.

Interactive error handling is one of the reasons that there is no clear distinction between development and production in scientific computing. When interfacing to analysis modules it is important that the role of errors is understood and recognised. An API must be able to handle exceptions and report error messages to the user, and certainly not crash the system. This aspect is not well understood outside of our community. Some popular commercial products seem to have propagated the belief that data analysis comes down to applying a magical tool to a dataset. However systems that implement such canned analyses don't do justice to the wide range of methods that statistics has to offer. Support for interactive error handling is therefore a crucial part of data analysis and embedded scientific computing.



\subsection{Security and resource control}

Somewhat related to the above are special needs in terms of security. Most statistical software currently available is primarily designed for interactive use on the local machine. Therefore access control is not considered an issue and the execution environment is entirely unrestricted. However in a context of shared or public embedded services we need to be able to implement security policies to prevent malicious or excessive use of resources. This in itself is not a unique problem. Most scripting languages such as \texttt{php}, \texttt{python} or \texttt{bash} do not enforce any access control and assume security will be implemented on the application level. However there are two domain specific aspects that complicate the problem. 

The first issue is that statistical software can be demanding and greedy with hardware resources. Numerical methods are expensive both in terms of memory and CPU. Fair use policies are not really feasible because excessive use of resources often happens unintentionally. For example, an overly complex model specification or algorithm getting stuck might end up consuming all available memory and cpu until manually terminated. When this happens on the local machine, the user can easily interrupt the process prematurely by sending a \texttt{SIGINT}, i.e. pressing \texttt{CTRL+C} or \texttt{ESC}. However in a shared environment this needs to be regulated by the system. Embedded scientific computing requires technology and policies that can manage and limit memory allocation, cpu cycles, disk space, concurrent processes, network traffic, etc. The degree of flexibility offered by implementation of resource management is an important factor in the scalability of a system. Fine grained control over system resources consumed by individual tasks allows for serving many concurrent users without sacrificing reliabilty. 

The second domain specific security issue is caused by the need for arbitrary code execution. A traditional application security model is based on user role privileges. For example in a standard web application, only a developer or administrator can implement and deploy actual code. The application merely exposes predefined functionality; users are not allowed to execute arbitrary code on the server. Any possibility of code injection is considered a security vulnerability and when found the server is potientially compromised. However as already mentioned, lack of segregation between users and developers in statistics gives limited use to applications that restrict users to predefined scripts and canned services. To support actual data analysis, the user needs access to the full language lexicon to freely explore, debug, model and visualize data. The need for arbitrary code execution disqualifies user based privileges and demands a more sophisticated security model.



\subsection{Reproducible research}

Replication of findings is one of the main principles of the scientific method. In quantitative research, a necessary condition for replication is reproducibiltity of results. The goal of reproducible research is to tie specific instructions to data analysis and experimental data so that scholarship can be recreated, better understood and verified \citep{cranRR}. Even though the ideas of replication are as old as science itself, reproducibility in scientific computing is still in its infancy. Tools are available that assist users in documenting their actions, but to most researchers these are not a natural part of their daily workflow. However the importance of replication in data analysis is increangly recognised and support for reproducibility is becoming more influential in the design of statistical software.

Reproducibility changes what constitutes the main product of an analysis. Rather than solely output and conclusions, we are interested recording and publishing the entire \emph{process}. This includes all data, code and results but also external software that was used arrive at the results. Therefore, reproducibility puts high requirements on software versioning. More than in other domains it is crucial that we diligently archive and administer the precise versions or branches of all scripts, packages, libraries, plugins that were somehow involved in the process. If an analysis involves randomness, it is also important that we keep track of which seeds and random number generators were used. In the current design of statistical software, reproducibility was mostly an afterthought and has te be taken care of manually. In practice it is tedious, expensive and error-prone. There is a lot of room for improvement for systems to incorporate reproducible practices as a natural part of the data analysis process.

Whereas reproducibility in statistics is acknowledged from a transparency/accountability point of view, it has enourmous potential to become much more than that. There are obvious parallels between reproducible research and revision control in source code management systems. Technology for automatic reproducible data analysis could revolutionize scientific collaboration, similar to what \texttt{git} has done for software development. A system that keeps track of each step in the  analysis process like a ``commit" in software versioning would make peer review, forking or follow-up analysis more practical and accessible. A scientific publication would no longer be considered an end product, but rather the starting point of scientific debate. When colleagues, supervisors or reviewers can easily reproduce results, test alternative hypotheses or recycle data, we achieve greater trustworthiness but also multiply return on investment of our work. Finally an open kitchen can help facilitate more natural ways of learning and teaching statistics. Rather than relying on general purpose textbooks with artificial examples, scholars directly study the practices of prominent researchers to understand how methods are applied in the context of data and problems as they appear specifically in their area of interest.


\section{The state problem}

Management of \emph{state} is a fundamental principle around which digital communications are designed. We distinguish \emph{stateful} and \emph{stateless} communication. In a stateless communication protocol, interaction involves independent request-response messages in which each request is unrelated by any previous request \citep{hennessy2012computer}. Because the messages are independent, there is no particular ordering to them, and requests can be performed concurrently. Examples of stateless protcols include the Internet Protocol (\IP), and the Hypertext Transfer Protocol (\HTTP). A stateful protocol on the other hand consists of an interaction via an ordered sequence of interrelated messages. The specification typically prescribes a specific mechanism for initiating and terminating a persistent \emph{connection} for information exchange. Examples of stateful protocols include the Transmission Control Protocol (\TCP) or File Transfer Protocol (\texttt{FTP}).

%Different notions of state can exist in different layers of communications. For example \TCP provides stateful connections in a layer on top \texttt{IP}, by specifying procedures for labeling and ordering \IP messages. Conversely, \HTTP is a stateless application layer that builds on \TCP. When using \HTTP, requests are independent of each other and not part of a persistent connection. However, many websites that use \HTTP do require some notion of state, for example to distinguish between requests from various authenticated users. In such applications, the client typically includes a special session-key in the payload of each \HTTP request. This key a unique value that helps the server determine which requests were part of one and the same session which allows the server to retain the state of each such user session.

In most data analysis software, the user controls a interactive session through a console or \GUI, with the possibility of executing a predefined sequence of operations in the form of a \emph{script}. Scripts are useful for publishing code or restoring state, however the most common and powerful way to use the software is interactively. In this respect, scientific computing is not unlike to a \emph{shell} interface to the operating system. Interactivity in scientific computing makes managment of state the most fundamental and challenging part of the interface design. When moving from a \UI to \API perspective, supporting statefulness become substantially more complicated. Existing bridges to \R have implemented one of two basic approaches discussed below. 

\subsection{Stateless solutions: predefined scripts}

The easiest solution is to not incorporate state on the level of the interface, and limit the system to predefined scripts. This is the standard approach in traditional web development. The web server exposes a parameratized service which generates dynamic content by calling out to a script on the system via \CGI. Any support for state has to be implemented manually in the application layer, e.g. by writing code that stores values in a database. For \R, we could use \texttt{rApache} \citep{horner2013rapache} to develop this kind of applications in very slimilar way as when we would use e.g. \texttt{php}. This works great for relatively simple applications that expose limited, predefined functionality. Scripted solutions give the developer flexibility to freely define input and output that are needed for a particular application. For example, we could design a service that generates some plot based on a couple of input parameters and returns a fixed size png image. Because scripts are stateless, multiple requests can be performed concurrently. A lot of the early work in this research has been based on this approach, which is a nice starting point but gets increasingly tricky once we move to more sophisticated applications.

The main limitation of this approach is that to support basic interactivity, retention of state needs to be implemented on the application level. However due to the complexicty of objects and data, this is much more involved than it is in e.g. \texttt{php}. For example a minimal application in statistics consists of the user uploading some data, performing some data manipulations and then creating a model, plot or report. When using scripts, the application developer needs to implement systems to manage and distinguish requests from various users and sessions, and store/load intermedate results in a database or disk. Doing this properly requires reasonably advanced \R and software engineering skills, and rapidly increases complexity as the application gets extended with additional scripts. Moreover storing objects/data on disk can introduce performance overhead. Because these problems will recur for almost any statistical application, we could benefit greatly from a system that supports retaining state by design.

Moreover predefined scripts are problematic because they divide developers and users in a way that is not very natural for scientific computing. The power of scripts in traditional web development is that they give give a lot of flexibility to the developer and very little to the user, which prevents malicous use of services. However in scientific computing, a script often  merely serves as a starting point for analysis. The user wants to be able to modify the script, or look at the data in another way by trying additional methods or different procedures. A system where clients can only perform prescripted actions severly handicaps the user and creates a lot of work for developers: because all functionality has to be prescripted, they are in charge of designing and implementing each possible action the user might want to perform. This is impractical for statistics because of the infinite amount of operations that can be performed on a dataset. For these reasons, the script approach does not scale well to many users or more complex applications.

\subsection{Stateful solution: client side process management}

Most existing bridges to \R take a stateful approach. Tools such as \texttt{Rserve} \citep{urbanek2013rserve} and \texttt{shiny} \citep{shiny} give each client a low-level interface to a private \R process over a (web)socket. This provides clients with freedom of running arbitrary \R code, which is great for implementing for example a webbased console or \texttt{IDE} such as \texttt{RStudio}. However the main problem with existing stateful solutions is lack of interoperability. Because these tools are in essense a remote \R console, they do not specify any standardized interface for calling methods, data I/O, etc. The advantage of the scripts approach discussed earlier was that clients simply pass parameters and the server returns output defined in the script such as a png image. However a low-level network interface requires extensive knowledge of \R internals to communicate. The client needs know how to call \R methods, interpret \R data structures, capture graphics, etc. These bridges are typically intended to be used in combination with dedicated clients. The \texttt{shiny} server for example comes with a set of predifined client widget templates that can be customized from within \R. The interface is not designed for integration with non-shiny software clients.

Stateful bridges also introduce some other difficulties. Systems that allocate a private \R process for each client can not support concurrent requests within a session. Each incoming request has to wait until the previous requests are finished for the process to become available. In addition to suboptimal performance, this can also be a source of instability. Procedures in data analysis are often unpredictable, and sometimes the \R process gets stuck or raises an unexpected error. In a local console we can easily interrupt or recover, but in an embedded system the server might become unresponsive causing the application to crash. Moreover, stateful servers are expensive and inefficient in terms of memory allocation. The server has to keep each \R process alive for the full duration of a session, even when idle. Memory that is in use by any single client does not free up until the user closes the application. This is particulary unfortunate because memory is often the main bottleneck in data intensive applications of scientific computing. Moreover, connectivity problems or ill behaved clients require the system to implement mechanisms to timeout and terminate inactive processes, or save and restore an entire session.

\subsection{OpenCPU: functional state}

We can take the best of both worlds by abstracting the notion of state to a higher level. Interactivity and state in \OpenCPU is provided through persistency of \emph{objects} rather then a persistent \emph{process}. As it turns out, this is a natural and powerful definition of state within the functional paradigm. Functional programming emphasizes that output from methods depends only on their inputs and not on the program state. Therefore functional languages can support state without keeping an entire process alive: merely retaining the objects is sufficient. As was discussed before, this has obvious parallels with mathematics, but also maps beautifully to stateless protocols such as \HTTP. The notion of state as the set of session objects is already native to \R as is apparent from the \texttt{save.image} function. Exploiting this notion of state for \RPC results in an intuitive \API that allows us to get the benefits of both traditional stateless and statefull approaches without introducing additional complexity. This simple observation provides the foundations for a very flexible and efficient stateful \RPC system.

The way this works in \OpenCPU is as follows. The \OpenCPU \API defines a mapping between \HTTP requests to \R function calls. An \HTTP \texttt{POST} request to a functional endpoint results in a function call where the request parameters are mapped to the function arguments. After executing the function call, \OpenCPU stores all outputs (such as return value, graphics or files) on the server and a session \ID is given to the client. The session \ID can be used to control these outputs on the server in future requests. For example, a client can retrieve outputs in various formats, share them with others, or use stored \R objects as arguments in subsequent function calls. Hence to build an interactive application, the client simply performs function calls by passing around \ID's representing objects on the server, even though the individual requests are technically stateless. Apart from reduced complexity, this system makes parallel computing and asynchronous requests a natural part of the interaction. For example to compute $f(g(x),(h(y))$, the client could perform \RPC requests for $g(x)$ and $h(y)$ simultaneously and pass the resulting output \ID's to $f()$ in a second step. In an asynchronous client language such as \texttt{JavaScript} this is so natural that it does not require any effort from the user or application developer.

One important detail is that the \OpenCPU \API deliberately does not prescribe \emph{how} the server should implement storing of objects. The \API only specifies a system for performing \R function calls over \HTTP and assigning \ID's to objects. Different server implementations can use different strategies for retaining such objects. A native implementation simply serializes objects to a directory on the server and immideately kills the process. This is safe and easy, however saving data on disk can be slow. A more sophisticated implementation could keep objects in memory for a while longer, either by keeping the \R process alive or through some sort of in-memory database or memcached system. This nicely illustrates the kind of optimization that we can achieve by carefully decoupling server and client components.

\section{The OpenCPU system}

The OpenCPU system defines an API for embedded scientific computing. The system builds on \R, however it can easily be generalized to other languages. The specification describes important high-level domain logic such as calling functions, running scripts, rendering \LaTeX \xspace documents, access to data, manual pages and  management of files and objects. Clients need no knowledge of \R: low-level concepts and concerns such as process management or code evaluation are abstracted. The ambition is a simple unambiguous interface to statistical operations that clients can work with, but leave language details and implementation choices as much as possible up to the server. Thereby responsibility of client and server are clearly seperated, which is essential for software development. New clients or applications updates can be implemented without affecting the server, and different servers can optimize within their strengths and limitations. An unambiguous API ensures that applications developed and tested on one server will work on any other server as well. 

As part of the research, two reference implementations were developed. The \R package \texttt{opencpu} builds on the \texttt{httpuv} web server (another \R package) to implement a \emph{single-user server} which runs within an interactive R session on any platform. The \emph{cloud server} is a multi-user implementation based on \texttt{Linux} and \texttt{rApache}. The latter yields better performance and has advanced security and configuration options, however it requires a dedicated Ubuntu server. One major difference between these implementations is how they handle concurrency. Because \R is single threaded, \texttt{httpuv} handles only one request at a time. Additional incoming requests are automatically queued and executed in succession using the same process. The cloud server on the other hand takes advantage of multi-processing in the Apache2 web server to handle concurrency. This implementation uses forks of the R process to serve concurrent requests immediately with little performance overhead. 

The differences between the cloud server and single user server are invisible to clients. The OpenCPU API provides a standard interface to either implementation. Other than varying performance, applications will behave the same regardless of which server is used. This shows the strenght of the system: applications can be developed locally using the single user server and published later on a shared high-performance cloud server.

\subsection{About HTTP}

One of the major strengths of OpenCPU is that it builds on the Hypertext Transfer Protocol (HTTP). HTTP is the most widely used appliation protocol on the internet, and the foundation of data communication in browsers and the world wide web. The protocol is defined in great detail, very mature and widely implemented. It provides all functionality required to build modern applications and has recently gained popularity for web APIs as well. 

The benefit if using a standardized application protocol such as HTTP is that a lot of funtionality gets built-in by design. HTTP has excellent mechanisms for authentication, caching, distribution, etc, which we can take advantage of with very minimal effort. This allows us to focus on the domain logic of scientific computing. Because general purpose application functionality exists on the level of the protocol, the OpenCPU API itself shines in simplicify. This is an enormous advantage over many other \R interfaces, which at best implement custom ad-hoc mechanisms for such functionality. In HTTP the solutions have been available for a long time and are already implemented in every client. This reduces risks of design flaws and makes our system more accessible and easier to adopt by 3rd party clients with limited knowledge of \R.

The obvious drawback is that we need to work within the restrictions of the protocol. One specific challenge discussed in a subsequent section is management of state. Because HTTP is stateless, it is not an option to simply provide access to an \R process over a socket such as done in \texttt{RServe} or \texttt{Shiny}. Instead the OpenCPU API needs to spend some additional effort in describing management of \emph{resources}. However this restriction is not merely a limitation but actually an important requirement for concurrency. The result is an elegant system for interacting with \R methods that naturally supports asynchronous requests and parallel computing. 


\subsection{Reproducbility by design}

Besides exposing high level logic to scientific computing, OpenCPU tries to incorporate the concept of reproducibility by design: in addition to results, the system automatically stores all input and output from all procedures. Hence, for each resource on the system, clients can lookup the code, data, warnings and packages that were involved in the creation of an object. Thereby each result can easily be recalculated, which forms a powerful foundation for reproducible practices. However this can also be used for other purposes. For example, if a procedure fetches dynamic data from an external resource to generate a model or plot, we can use reproduction to \emph{update} the model or plot with the latest data.

The importance and power of reproduction in scientific computing cannot be over-emphasized. Reprodibles could become the building block of collaborative science, analogous to a "commit" in software versioning. Reproducible resources make it natural for researchers to publish, fork, learn and teach results. The transition to embedded analysis provides a great opportunity to include better support for reproducible research in our software. By incorporating the notion of reproduction in the API, OpenCPU sets an example that turns reproduction into a native aspect of the interaction with the system. The current implementation is pretty basic, but it can be extended and is a proof of concept that illustrates the direction we should be headering.  

\subsection{Less is more}

\begin{quote}
\emph{``perfection is reached not when there is nothing left to add, but when there is nothing left to take away''} --- Antoine de Saint-Exupry
\end{quote}

\vspace{8pt}

%KISS (\emph{``Keep it simple, stupid''}) is a popular ancronym coined in the 1960s by the U.S. Navy \citep{victor2007concise} for the design principle stating that systems work best if they are kept simple rather than made complicated. Some other frequently cited testimonials of similar observations include \emph{``less is more''} (Mies van der Rohe), \emph{``Simplicity is the ultimate sophistication''} (Leonardo da Vinci), and \emph{``perfection is reached not when there is nothing left to add, but when there is nothing left to take away''} (Antoine de Saint-Exupry). 

\noindent Management of complexity is one of the most challenging aspects of modern software development. When pressure and temptation to expand functionality is not balanced by a constant strive for simplicity, a project is heading for failure. Each newly introduced feature demands additional testing, maintenance and support down the road, which can rapidly turn into a sinkhole of developer time. Moreover costs, overhead and limitations introduced by unwanted stuff make bloated software less attractive. Also programs gets increasingly difficult to learn and understand if important principles are masked by smoke screens, bells and whistles of vaguely related bonus features. Therefore finding a minimal generalizable solution to address a particular problem or market is key to software simplicity. However this is rarely easy. It requires careful deliberation of the role, scope and priorities of a system; of what it \emph{is} and what it should or should not do. Incomplete solutions that leave important problems unsolved are unreliable and impractical. However components that are overly involved might conflict with other pieces, and complicate the system. The quest for a clean and elegant design often consists of many iterations of adding, removing, abstracting, rethinking and refactoring the system.

For OpenCPU a lot of thought has gone into what are the fundamental building blocks of embedded scientific computing. The system should solve recurring problems and give structure to applications without imposing unnecessary limitations. The OpenCPU API sets out important guidelines on what is considered the responsibility of respectively the developer (client-side), statistician (server-side) and system itself (middle layer). For example, all \R functions and scripts accessible through the OpenCPU system. However, the API only specifies  procedures for remotely \emph{calling} such functions and \emph{retrieving} results. The specification does not mention any details on the behavior and implementation of functions or which data is exchanged. Other concerns such as authentication can be solved on the layer of the application protocol (i.e. HTTP). Hence even though useful, these are not fundamental to scientific computing specifically and therefore need no specification in our API. Finally the system has some meta-functionality such as reproducibility mentioned earlier. It is tempting to include more goodies such as permanent storage, system administration or front-end widgets. However, such features are not specific to scientific computing and can easily be realized as separate, independent software pieces. Therefore they do not need to be part of the specification either. 

OpenCPU is what remains after all non essential parts are taken away from the API. It defines a middle layer that addresses the core problems of embedded scientific computing, and nothing else. It specifies a system for management of resources and remote function calls over HTTP that isolates scientific computing from other parts of the system. It prescribes several data interchange formats, but no specific data types or structures. The system is intended to be used \emph{in conjunction} with other software such as database, web framework or administration tools. It is a powerful building block for stacks and applications with embedded analysis or visualization. 

\subsection{History of OpenCPU}

The OpenCPU specification outlined in this paper was not drafted overnight, but evolved as many iterations of trial and error. Initial inspirations were drawn from recurring problems in developing various \R web applications with \texttt{rApache}. Experiences from these applications revelealed fundamental challenges and domain logic of embedded scientific computing. The accumulated knowledge shaped a vision on what a system facilitating this entails that would eventually turn into OpenCPU.

After about a year of internal development, the first public version of OpenCPU appeared in August 2011. These alpha and beta versions were picked up by several early adopters, both from industry and academia, some of which are still in production today. The problems and critisim generated by these versions provided great feedback and revealed some fundamental design problems. At the same time exciting developments were going on in the \R community, in particular the rise of R-studio and introduction of influential \R packages \texttt{knitr}, \texttt{evaluate} and \texttt{httpuv}. These technologies provided more flexible and reliable foundations for OpenCPU than what was available than before. After a redesign of the API and a complete rewrite of the code, OpenCPU version  1.0 was released in August 2013. This version emphasizes simplicity and scalability by removing all non-essential functionality. What remains is an API that captures the core domain logic of embedded scientific computing. By making use of native features found in HTTP, the this version is flexible and extensible without getting bloated or overly complex.


\section{The OpenCPU API specification}

Core: manage objects and perform remote function calls. Very similar to R session.
All else follows naturally from HTTP.

\subsection{Methods}

- Perform remote function calls: POST
- Retrieve objects: GET

\subsection{Status Codes}

\subsection{Media Types}

Data can be posted using the following formats:
url-encoded, formdata, application/json

For retrieval, format is requested through URL, rather than Accept header.
I.e. each format is promoted to a separate resource. 

 - because formats are not as interchangable as some other apis. 
 - to link directly to particular format, e.g. embed PNG or PDF file
 - Arguments depend on the format. 

\subsection{Response Headers}

Important:
- content-type
- cache-control
- location

Other:
- x-ocpu-session

Misc (debuggin)
- x-ocpu-**

\subsection{Package Resources}

 - root url: dynamic. Default to /ocpu in current implementations.  
 - libraries
   - packages:
     - data
     - objects
     - manual pages
     - files (scripts, web pages, etc)
     
\subsection{Session Resources}

 - session library
 - sessions contain:
   - objects
   - graphics
   - files
   - source
   - stdin
   - stdout
   - console

\subsection{Static File Resources}

Used only to read/execute scripts.
Clients sends no arguments.
Interesting for reproducible research.


\section{Input and output Formats}

Input and output of data formats are more somewhat more specific to R.
Input are used for function arguments. Layer on the media types.
Outputs formats used to retrieve objects, manuals, graphics.
Each output format has arguments to control format.

\section{Apps}

Native R packaging + API = webapps.
JavaScript library makes web clients very easy.



\bibliographystyle{plainnat}
\bibliography{thesis}
\end{document}
