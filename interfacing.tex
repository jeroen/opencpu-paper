%meta
\documentclass{article}
\author{Jeroen Ooms}
\title{The OpenCPU System:\\ Towards a Universal Interface for Scientific Computing}

%some packages
\usepackage{url}
\usepackage{fullpage}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage[round]{natbib}
\usepackage{fancyvrb}
\usepackage[toc,page]{appendix}
\usepackage{breakurl}

%support for accents
\usepackage[utf8]{inputenc}

%use vspace instead of indentation for paragraphs
\usepackage{parskip}

%extra line spacing
\usepackage{setspace}
\setstretch{1.2}

%custom commands
\newcommand{\SQL}{\texttt{SQL}\xspace}
\newcommand{\CGI}{\texttt{CGI}\xspace}
\newcommand{\R}{\texttt{R}\xspace}
\newcommand{\HTTP}{\texttt{HTTP}\xspace}
\newcommand{\TCP}{\texttt{TCP}\xspace}
\newcommand{\IP}{\texttt{IP}\xspace}
\newcommand{\GUI}{\texttt{GUI}\xspace}
\newcommand{\CLI}{\texttt{CLI}\xspace}
\newcommand{\UI}{\texttt{UI}\xspace}
\newcommand{\API}{\texttt{API}\xspace}
\newcommand{\RPC}{\texttt{RPC}\xspace}
\newcommand{\ID}{\texttt{ID}\xspace}
\newcommand{\OpenCPU}{\texttt{OpenCPU}\xspace}

%actual doc
\begin{document}

\maketitle

\begin{abstract}
A vital part of facilitating embedded scientific computing is specification of the interface. Clients or systems integrating analytical components require a simple well interface that exposes desired functionality and hides implementation details. Currently, most statistical software is designed for UI rather than API interaction which presents some challenges when turning methods into components. Because scientific computing is highly interactive, the role of \emph{state} is the central problem in the design of an interface. This paper introduces OpenCPU: a system for embedded scientific computing based on R. The OpenCPU API is described in detail and used to exemplify interoperability of statistical methods by decoupling application from statistical computation. Because OpenCPU leverages native features of HTTP, it can easily be extended and no knowledge of R is required to implement clients. We finish by showing how the OpenCPU makes it trivial to turn R packages into webapps.
\end{abstract}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\vspace{8pt}

\begin{quote}
\emph{Premature optimization is the root of all evil.} --- Donald Knuth
\end{quote}

\section{Introduction}

- what is embedded scientific computing
- why is it increasingly important
- scale


%Methods for scientific computing are typically implemented in specialized packages assisting the statistician in all facets of the data analysis process. Some products are optimized for use via a command line interface (\CLI) whereas others focus on the graphical user interface (\GUI), yet most emphasize the user interface (\UI) as primary point of interaction. However the changing role of data analysis in modern society is demanding better interoperability of statistical software. Embedding statistical methods in the next generation of software systems requires better application programming interaces (\API). This means moving away from manual solutions towards component-based analysis units. This research is a piece in a series of contributions advocating for a more flexible and integrated approach to data analysis. Throughout this paper we refer to this under the umbralla term of \emph{embedded scientific computing}.

%A vital part of facilitating embedded scientific computing is specification of the interface. Clients or systems integrating analytical components require a well defined \API that exposes desired functionality while abstracting language specifics and implementation details. Defining an interoperable entry point allows servers to expose computational methods, and clients to call these methods. Such a universal interface separates the domain logic of scientific computing from application logic and lower-level logistics. 

%An interesting analogy is provided by database interfaces. For example, the \texttt{SQL} language provides an interoperable interface for managing data held in a relational database management system (\texttt{RDBMS}). \texttt{SQL} consists of a data definition language and a data manipulation language that separates client and server responsibilities. The standard interface allows for implementing clients and applications building on relational data without any database implementation knowledge. Many \texttt{SQL} servers such as \texttt{MySQL} and \texttt{MS SQL Server} implement this standard for various platforms and languages and providing different optimization and configuration options. Naturally, some \texttt{SQL} extensions and dialects exist. But the core of the language is widely implemented and has greatly contributed to the wide adoption of relational databases.

\subsection{The need for abstraction}

Previous work in embedded scientific computing has mostly been limited to low-level tools for connecting statistical software to general purpose environments. As a result, ``bridges'' are available to execute an \R script or process from inside most popular languages. For example, \texttt{JRI} \citep{rjava}, \texttt{RInside} \citep{eddelbuettel2011rcpp}, \texttt{rpy2} \citep{gautier2008rpy2} or \texttt{RinRuby} \citep{dahl2008rinruby} can be used to call \R from respectively \texttt{Java}, \texttt{C++}, \texttt{Python} or \texttt{Ruby}. \cite{heiberger2009r} provide a set of tools to execute \R code from \texttt{DCOM} clients on Windows, mostly to support running \R in Microsoft Excel. The Apache2 module \texttt{rApache} (\texttt{mod\_R}) makes it possible to run R scripts from the Apache2 web server \citep{horner2013rapache}. The \texttt{littler} program provides hash-bang capability for \R, as well as simple command-line and piping use on \texttt{unix} \citep{littler}. Finally, \texttt{Rserve} is \texttt{TCP/IP} server which provides low level access to an R session over a socket \citep{urbanek2013rserve}. 

However even though these tools have been available for several years, they have not been able to facilitate the big break through of \R as a ubiquitous statistical engine. Given the enourmous demand for analysis and visualization these days, the adoption of \R for embedded scientific computing is actually quite underwhelming. In my experience, the primary cause for the limited success is that low-level bridges are difficult to implement, do not scale very well, and leave the most challenging problems unsolved. Substantial plumbing and expertise of \R internals is required for building actual applications on these tools. Clients are supposed to generate and push \R syntax, make sense of \R's internal \texttt{C} structures and write their own framework for managing jobs, graphics, security, data interchange, etc. In practice this is very difficult, results in messy code, and often leads to unstable applications when important issues were not fully taken care of. The tools are also impractical from a human point of view. Building a web application with for example \texttt{JRI}, \texttt{RInside} or \texttt{Rserve} requires a web developer that is also an expert in \R, as well as \texttt{C++} or \texttt{Java}. Because \R is such a domain specific language, this combination of skills is very rare. 

What is needed to scale up embedded scientific computing is a system that decouples the application layer from the computational back-end, similar to how e.g. \texttt{SQL} separates the application and database. A well defined separation of concerns allows for independent development of client and server components, by different people with different background and expertise. Idealy, the system interface should describe domain logic of scientific computing and abstract implementation details as much as possible. This gives clients a universal point of interaction to perform data operations without any specialized knowledge about the computational language or infrastructure. At the same time, statisticians can expose analysis methods for use in applications without having to learn web development or other tools. Careful isolation of components along the lines of domain logic would allow developers to focus on their expertise using their tools of choice. This vision forms the starting point of this research. 


\subsection{Scope}

This paper provides an introduction to the topic of embedded scientific computing. The presented ideas are largely derived from personal experiences of developing systems and applications with integrated analysis and vizualization. We identify primary challenges and opportunities in transitioning from a \UI to \API paradigm, and moving beyond ad-hoc cross language bindings towards foundations for scalable component based data analysis. Rather than focusing on technical problems, our approach puts heavy emphasis on exploring the domain logic of the field along the lines of \citep{evans2004domain}. Scalable components require meticulous separation of concerns, and a significant share of this work is devoted to describing the concerns of scientific computing. To this end, our journey starts with an extensive review of the purpose, problems and practices of data analysis, and the implications for embedded components. The goal is to study the conceptual model of scientific computing and carefully separate it from implementation or application. This provides the requirements for a system that facilitates embedded scientific computing, and sets the stage for an interface definition.


\subsection{The \OpenCPU system}

The \OpenCPU system is used as a reference implementation throughout this paper. \OpenCPU defines an \HTTP \API that builds on \emph{The R Project for Statistical Computing} \citep{R}, for short: \R. The \R language is the obvious candidate for a first implementation of this kind. It is currently the most popular open source software for data analysis and considered by many statisticians as the \emph{de facto} standard of data analysis. The huge \R community provides both the tools and use-cases needed to develop and experiment with this new approach to scientific computing. Currently only \R has the required scale and foundations to really put our ideas to the test. However we want to stress that even though our research and the \OpenCPU system are inspired by and tailored to the way things work in \R, the approach should generalize quite naturally to other computational back-ends. We try to capture the general logic of data analysis rather than that of a particular language, but in practice it is very difficult to study or discuss software without an implementation. The \OpenCPU system illustrates what an abstracted interface to scientific computing could look like. The implementation allows us and others to put this new approach into practice and get first hand experience with the problems and opportunities in this unexplored field. 

%For this reason the paper discusses concerns of scientific computing mostly in the context and terminology of \R.  Therefore we rely on \R to illustrate and exemplify the concepts of scientific computing. For example, the conventional container format for namespacing a collection of data and functions in \R is a \emph{package}. Other products might use different mechanics and jargon for combining and publishing a set of objects. However any system needs some notion of namespaces and dependencies analogous to packages in \R. For sake of simplicity this paper discusses such concepts mostly from the way they take shape in \R. 



\input{domainlogic}

\input{stateproblem}



\section{The OpenCPU system}

The OpenCPU system defines an API for embedded scientific computing. The system builds on \R, however it can easily be generalized to other languages. The specification describes important high-level domain logic such as calling functions, running scripts, rendering \LaTeX \xspace documents, access to data, manual pages and  management of files and objects. Clients need no knowledge of \R: low-level concepts and concerns such as process management or code evaluation are abstracted. The ambition is a simple unambiguous interface to statistical operations that clients can work with, but leave language details and implementation choices as much as possible up to the server. Thereby responsibility of client and server are clearly seperated, which is essential for software development. New clients or applications updates can be implemented without affecting the server, and different servers can optimize within their strengths and limitations. An unambiguous API ensures that applications developed and tested on one server will work on any other server as well. 

As part of the research, two reference implementations were developed. The \R package \texttt{opencpu} builds on the \texttt{httpuv} web server (another \R package) to implement a \emph{single-user server} which runs within an interactive R session on any platform. The \emph{cloud server} is a multi-user implementation based on \texttt{Linux} and \texttt{rApache}. The latter yields better performance and has advanced security and configuration options, however it requires a dedicated Ubuntu server. One major difference between these implementations is how they handle concurrency. Because \R is single threaded, \texttt{httpuv} handles only one request at a time. Additional incoming requests are automatically queued and executed in succession using the same process. The cloud server on the other hand takes advantage of multi-processing in the Apache2 web server to handle concurrency. This implementation uses forks of the R process to serve concurrent requests immediately with little performance overhead. 

The differences between the cloud server and single user server are invisible to clients. The OpenCPU API provides a standard interface to either implementation. Other than varying performance, applications will behave the same regardless of which server is used. This shows the strenght of the system: applications can be developed locally using the single user server and published later on a shared high-performance cloud server.

\subsection{About HTTP}

One of the major strengths of OpenCPU is that it builds on the Hypertext Transfer Protocol (HTTP). HTTP is the most widely used appliation protocol on the internet, and the foundation of data communication in browsers and the world wide web. The protocol is defined in great detail, very mature and widely implemented. It provides all functionality required to build modern applications and has recently gained popularity for web APIs as well. 

The benefit if using a standardized application protocol such as HTTP is that a lot of funtionality gets built-in by design. HTTP has excellent mechanisms for authentication, caching, distribution, etc, which we can take advantage of with very minimal effort. This allows us to focus on the domain logic of scientific computing. Because general purpose application functionality exists on the level of the protocol, the OpenCPU API itself shines in simplicify. This is an enormous advantage over many other \R interfaces, which at best implement custom ad-hoc mechanisms for such functionality. In HTTP the solutions have been available for a long time and are already implemented in every client. This reduces risks of design flaws and makes our system more accessible and easier to adopt by 3rd party clients with limited knowledge of \R.

The obvious drawback is that we need to work within the restrictions of the protocol. One specific challenge discussed in a subsequent section is management of state. Because HTTP is stateless, it is not an option to simply provide access to an \R process over a socket such as done in \texttt{RServe} or \texttt{Shiny}. Instead the OpenCPU API needs to spend some additional effort in describing management of \emph{resources}. However this restriction is not merely a limitation but actually an important requirement for concurrency. The result is an elegant system for interacting with \R methods that naturally supports asynchronous requests and parallel computing. 


\subsection{Reproducbility by design}

Besides exposing high level logic to scientific computing, OpenCPU tries to incorporate the concept of reproducibility by design: in addition to results, the system automatically stores all input and output from all procedures. Hence, for each resource on the system, clients can lookup the code, data, warnings and packages that were involved in the creation of an object. Thereby each result can easily be recalculated, which forms a powerful foundation for reproducible practices. However this can also be used for other purposes. For example, if a procedure fetches dynamic data from an external resource to generate a model or plot, we can use reproduction to \emph{update} the model or plot with the latest data.

The importance and power of reproduction in scientific computing cannot be over-emphasized. Reprodibles could become the building block of collaborative science, analogous to a "commit" in software versioning. Reproducible resources make it natural for researchers to publish, fork, learn and teach results. The transition to embedded analysis provides a great opportunity to include better support for reproducible research in our software. By incorporating the notion of reproduction in the API, OpenCPU sets an example that turns reproduction into a native aspect of the interaction with the system. The current implementation is pretty basic, but it can be extended and is a proof of concept that illustrates the direction we should be headering.  

\subsection{Less is more}

\begin{quote}
\emph{``perfection is reached not when there is nothing left to add, but when there is nothing left to take away''} --- Antoine de Saint-Exupéry
\end{quote}

\vspace{8pt}

%KISS (\emph{``Keep it simple, stupid''}) is a popular ancronym coined in the 1960s by the U.S. Navy \citep{victor2007concise} for the design principle stating that systems work best if they are kept simple rather than made complicated. Some other frequently cited testimonials of similar observations include \emph{``less is more''} (Mies van der Rohe), \emph{``Simplicity is the ultimate sophistication''} (Leonardo da Vinci), and \emph{``perfection is reached not when there is nothing left to add, but when there is nothing left to take away''} (Antoine de Saint-Exupéry). 

\noindent Management of complexity is one of the most challenging aspects of modern software development. When pressure and temptation to expand functionality is not balanced by a constant strive for simplicity, a project is heading for failure. Each newly introduced feature demands additional testing, maintenance and support down the road, which can rapidly turn into a sinkhole of developer time. Moreover costs, overhead and limitations introduced by unwanted stuff make bloated software less attractive. Also programs gets increasingly difficult to learn and understand if important principles are masked by smoke screens, bells and whistles of vaguely related bonus features. Therefore finding a minimal generalizable solution to address a particular problem or market is key to software simplicity. However this is rarely easy. It requires careful deliberation of the role, scope and priorities of a system; of what it \emph{is} and what it should or should not do. Incomplete solutions that leave important problems unsolved are unreliable and impractical. However components that are overly involved might conflict with other pieces, and complicate the system. The quest for a clean and elegant design often consists of many iterations of adding, removing, abstracting, rethinking and refactoring the system.

For OpenCPU a lot of thought has gone into what are the fundamental building blocks of embedded scientific computing. The system should solve recurring problems and give structure to applications without imposing unnecessary limitations. The OpenCPU API sets out important guidelines on what is considered the responsibility of respectively the developer (client-side), statistician (server-side) and system itself (middle layer). For example, all \R functions and scripts accessible through the OpenCPU system. However, the API only specifies  procedures for remotely \emph{calling} such functions and \emph{retrieving} results. The specification does not mention any details on the behavior and implementation of functions or which data is exchanged. Other concerns such as authentication can be solved on the layer of the application protocol (i.e. HTTP). Hence even though useful, these are not fundamental to scientific computing specifically and therefore need no specification in our API. Finally the system has some meta-functionality such as reproducibility mentioned earlier. It is tempting to include more goodies such as permanent storage, system administration or front-end widgets. However, such features are not specific to scientific computing and can easily be realized as separate, independent software pieces. Therefore they do not need to be part of the specification either. 

OpenCPU is what remains after all non essential parts are taken away from the API. It defines a middle layer that addresses the core problems of embedded scientific computing, and nothing else. It specifies a system for management of resources and remote function calls over HTTP that isolates scientific computing from other parts of the system. It prescribes several data interchange formats, but no specific data types or structures. The system is intended to be used \emph{in conjunction} with other software such as database, web framework or administration tools. It is a powerful building block for stacks and applications with embedded analysis or visualization. 

\subsection{History of OpenCPU}

The OpenCPU specification outlined in this paper was not drafted overnight, but evolved as many iterations of trial and error. Initial inspirations were drawn from recurring problems in developing various \R web applications with \texttt{rApache}. Experiences from these applications revelealed fundamental challenges and domain logic of embedded scientific computing. The accumulated knowledge shaped a vision on what a system facilitating this entails that would eventually turn into OpenCPU.

After about a year of internal development, the first public version of OpenCPU appeared in August 2011. These alpha and beta versions were picked up by several early adopters, both from industry and academia, some of which are still in production today. The problems and critisim generated by these versions provided great feedback and revealed some fundamental design problems. At the same time exciting developments were going on in the \R community, in particular the rise of R-studio and introduction of influential \R packages \texttt{knitr}, \texttt{evaluate} and \texttt{httpuv}. These technologies provided more flexible and reliable foundations for OpenCPU than what was available than before. After a redesign of the API and a complete rewrite of the code, OpenCPU version  1.0 was released in August 2013. This version emphasizes simplicity and scalability by removing all non-essential functionality. What remains is an API that captures the core domain logic of embedded scientific computing. By making use of native features found in HTTP, the this version is flexible and extensible without getting bloated or overly complex.


\section{The OpenCPU API specification}

Core: manage objects and perform remote function calls. Very similar to R session.
All else follows naturally from HTTP.

\subsection{Methods}

- Perform remote function calls: POST
- Retrieve objects: GET

\subsection{Status Codes}

\subsection{Media Types}

Data can be posted using the following formats:
url-encoded, formdata, application/json

For retrieval, format is requested through URL, rather than Accept header.
I.e. each format is promoted to a separate resource. 

 - because formats are not as interchangable as some other apis. 
 - to link directly to particular format, e.g. embed PNG or PDF file
 - Arguments depend on the format. 

\subsection{Response Headers}

Important:
- content-type
- cache-control
- location

Other:
- x-ocpu-session

Misc (debuggin)
- x-ocpu-**

\subsection{Package Resources}

 - root url: dynamic. Default to /ocpu in current implementations.  
 - libraries
   - packages:
     - data
     - objects
     - manual pages
     - files (scripts, web pages, etc)
     
\subsection{Session Resources}

 - session library
 - sessions contain:
   - objects
   - graphics
   - files
   - source
   - stdin
   - stdout
   - console

\subsection{Static File Resources}

Used only to read/execute scripts.
Clients sends no arguments.
Interesting for reproducible research.


\section{Input and output Formats}

Input and output of data formats are more somewhat more specific to R.
Input are used for function arguments. Layer on the media types.
Outputs formats used to retrieve objects, manuals, graphics.
Each output format has arguments to control format.

\section{Apps}

Native R packaging + API = webapps.
JavaScript library makes web clients very easy.



\bibliographystyle{plainnat}
\bibliography{thesis}
\end{document}
