% !TEX root = main.tex
\section{Introduction}

Methods for scientific computing are traditionally implemented in specialized software packages assisting the statistician in all facets of the data analysis process. A single product typically includes a wealth of functionality to interactively manage, explore and analyse data, and often much more. Some products such as \R or \texttt{STATA} are optimized for use via a command line interface (\CLI) whereas others such as \SPSS focus mainly on the graphical user interface (\GUI). However increasingly many users and organisations wish to integrate statistical methods into third party software. Rather than working in a specialized statistical environment, methods to analyze and visualize data get incorporated into pipelines, web applications and big data infrastructures. This way of doing data analysis requires a different approach to statistical software which puts more emphasis on modularity, interoperability and programmable interfaces rather than \UI interaction. Throughout this paper we refer to this approach to statistical software as \emph{embedded scientific computing}.

Previous work in embedded scientific computing has mostly been limited to low-level tools for connecting statistical software to general purpose environments. For \R, bindings and bridges are available to execute an \R script or process from inside most popular languages. For example, \texttt{JRI} \citep{rjava}, \texttt{RInside} \citep{eddelbuettel2011rcpp}, \texttt{rpy2} \citep{gautier2008rpy2} or \texttt{RinRuby} \citep{dahl2008rinruby} can be used to call \R from respectively \texttt{Java}, \texttt{C++}, \texttt{Python} or \texttt{Ruby}. \cite{heiberger2009r} provide a set of tools to execute \R code from \texttt{DCOM} clients on Windows, mostly to support running \R in Microsoft Excel. The Apache2 module \texttt{rApache} (\texttt{mod\_R}) makes it possible to run R scripts from the Apache2 web server \citep{horner2013rapache}. The \texttt{littler} program provides hash-bang capability for \R, as well as simple command-line and piping use on \texttt{unix} \citep{littler}. Finally, \texttt{Rserve} is \texttt{TCP/IP} server which provides low level access to an R session over a socket \citep{urbanek2013rserve}. 

However, even though bridging tools have been available for several years, they have not been able to facilitate the big break through of \R as a ubiquitous statistical engine. Given the enourmous demand for analysis and visualization these days, the adoption of \R for embedded scientific computing is actually quite underwhelming. In my experience, the primary cause for the limited success is that these bridges are hard to implement, do not scale very well, and leave the most challenging problems unresolved. Substantial plumbing and expertise of \R internals is required for building actual applications on these tools. Clients are supposed to generate and push \R syntax, make sense of \R's internal \texttt{C} structures and write their own framework for managing requests, graphics, security, data interchange, etc. Thereby concerns of scientific computing get intermingled with other parts of the system resulting in highly coupled software which is complex, hard to maintain and often unreliable. High coupling is also problematic from a human point of view. Building a web application with for example \texttt{Rserve} requires a web developer that is also an expert in \R, \texttt{Java} and \texttt{Rserve}. Because \R is a very domain specific language, such a combination of skills is very rare and expensive. 


\subsection{Separation of concerns}

%A helpful analogy for statisticians might be provided by \texttt{SQL} servers. \texttt{SQL} separates the management of relational data from other components through an interoperable language. 
What is needed to scale up embedded scientific computing is a system that decouples data analysis from other system components. Applications should be able to integrate statistical methods using modular analysis units without detailed knowledge about their implemenatation. Component based software engineering emphasizes the design principle of \emph{separation of concerns} \citep{heineman2001component}, which advocates that a computer program is split up into distinct pieces which each encapsulate a logical set of functionality behind a well defined interface. This allows for independent development of various components, by different people with different background and expertise. Separation of concerns is fundamental to the functional programming paradigm \citep{reade1989elements} as well as the design of service orriented architectures on distributed information systems such as the internet \citep{fielding2000architectural}. The principle lies at the heart of embedded scientific computing and holds the key to advancing distributed analysis systems.
%Concerns within an architecture are generally broader defined than within the context of a language, but the same rules and benefits hold. 

In order to develop a system that can separate concerns of scientific computing from other parts of the system, we need to ask ourselves the question: \emph{what are the concerns of data analysis?} This question does not have an easy answer. Over the years statistical softare has gotten highly convoluted by the inclusion of complementary tools that are useful but not neccessarily an integral part of computing itself. Separation of concerns requires us to extract the core logic of scientific computing and divorce it from all other apparatus. We need to define a conceptual model of data analysis that is independent of any particular application or implementation. Therefore, rather than discussing technical problems, this paper focuses entirely on studying the domain logic of the discipline along the lines of \cite{evans2004domain}. By exploring the purpose, problems and practices of the field we try to unveil the fundamental principles behind statistical computing. Along the way we highlight important problems and bottlenecks that require further attention in order to facilicate reliable and scalable analysis units.

The end goal of this paper is to work towards an interface definition for embedded scientific computing. An interface is the embodiment of separation of concerns and serves as a contract that formalizes the boundary across which separate components exchange information. The interface definition describes the concepts and operations that components agree upon to cooperate and how the communication is arranged. Through the interface we specify the functionality that a server has to implement, which parts of the interaction are fixed and which choices are specifically left at the discretion of the implementation. Ideally the specification should provide sufficient structure to develop clients and server components for scientific computing while minimizing limitations on how these can be implemented. An interface that carefully isolates components along the lines of domain logic allows developers to focus on their expertise using their tools of choice. It gives clients a universal point of interaction to integrate statistical methods without understanding the actual computing, and allows statisticians to implement methods for use in applications without knowing specifics about the application layer.

%- think in analogy of SQL describing relational data management

\subsection{The \OpenCPU system}

The \OpenCPU system is introduced in this paper as an example implementation that illustrates what an abstracted interface to scientific computing could look like. \OpenCPU defines an \HTTP \API that builds on \emph{The R Project for Statistical Computing}, for short: \R \citep{R}. The \R language is the obvious candidate for a first implementation of this kind. It is currently the most popular open source software for data analysis and considered by many statisticians as the de facto standard of data analysis. The huge \R community provides both the tools and use-cases needed to develop and experiment with this new approach to scientific computing. It is fair to say that currently only \R has the required scale and foundations to really put our ideas to the test. However we want to stress that even though the research and \OpenCPU system are colored by and tailored to the way things work in \R, the approach should generalize quite naturally to other computational back-ends. The \texttt{API} is designed to describe general logic of data analysis rather than that of a particular language. The main role of the software is to allow us and others to put this new approach into practice and get first hand experience with the problems and opportunities in this unexplored field. 

As part of the research, two \OpenCPU server implementations were developed. The \R package \texttt{opencpu} uses the \texttt{httpuv} web server \citep{httpuv} to implement a \emph{single-user server} which runs within an interactive R session on any platform. The \emph{cloud server} on the other hand is a multi-user implementation based on \texttt{Ubuntu Linux} and \texttt{rApache}. The latter yields much better performance and has advanced security and configuration options, however it requires a dedicated \Linux server. Another major difference between these implementations is how they handle concurrency. Because \R is single threaded, \texttt{httpuv} handles only a single request at a time. Additional incoming requests are automatically queued and executed in succession using the same process. The cloud server on the other hand takes advantage of multi-processing in the \texttt{Apache2} web server to handle concurrency. This implementation uses forks of the \R process to serve concurrent requests immediately with little performance overhead. The differences between the cloud server and single user server are invisible the client. The \API provides a standard interface to either implementation and other than varying performance, applications will behave the same regardless of which server is used. 
%The remainder of this paper focuses entirely on the logic of the system and does not treat implementation details in any further detail, apart from some details that are helpful to illustrate and exemplify the interface.


\subsection{History of \OpenCPU}

This paper starts with a discussion of domain logic of scientific computing and then progresses towards interface and implementation details. However the actual course of this research has mostly been the reverse order. The \OpenCPU interface and implementation co-evolved through several iterations of trial and error. While implementing and using the software we discover ambiguities in the specification, limitations of the design while getting punished for complexity. The quest for finding a set of functionality that balances strucure and simplicity has been influential to the ideas on the principal concepts of scientific computing. 

%For example the current \API specifies a system for requesting objects in a particular format, but does not regulate the structure and content of data interchange. This is a choice; an alternative option would have been to omit data interchange from the specification and leave the input and output format of a procedure up to the server implementation. However in practice different users or clients might support or prefer different data interchange formats. Therefore an \API that lets the client rather than the server pick the format of data interchange is more powerful. Yet another option would have been to go further than we do now and prescribe a system for specifying data structures using schemas. However in practice this is not very natural to scientific computing where languages are often weakly typed and define data structures relatively loosely. Other concerns such as authentication can be solved on the layer of the application protocol (i.e. HTTP). Hence even though useful, these are not fundamental to scientific computing specifically and therefore need no specification in our API. It is also tempting to include more goodies such as permanent storage, system administration or front-end widgets. However, such features are not specific to scientific computing and can easily be realized as separate, independent software pieces. Therefore they do not need to be part of the specification either. \OpenCPU is what remains after all non essential parts are removed from the \API. It specifies a system for management of resources and remote function calls over \HTTP that decouples scientific computing from other parts of the system. The system can easily be extended and is intended to be used in conjunction with other software such as database, web framework or administration tools.

The \OpenCPU system builds on several years of work, dating back to 2009. Initial inspirations were drawn from recurring problems in developing various \R web applications with \texttt{rApache} such as \citep{van2009stage}. Accumulated experiences from these projects shaped a vision on what is involved with embedded scientific computing that would eventually turn into \OpenCPU. After a year of internal development, the first public beta of \OpenCPU appeared in August 2011. This version was picked up by early adopters in both industry and academia, some of which are still in production today. The problems and suggestions generated from early versions were a great source of feedback and revealed some fundamental problems. At the same time exciting developments were going on in the \R community, in particular the rise of RStudio and introduction of powerful \R packages \texttt{knitr}, \texttt{evaluate} and \texttt{httpuv}. After a redesign of the \API and a complete rewrite of the code, \texttt{OpenCPU 1.0} was released in August 2013. By making better use of native features in \HTTP, this version is more flexible and extensible without getting bloated or overly complex. Further releases within the \texttt{1.x} branch have introduced additional server configurations and optimizations without any major changes to the \API.


