% !TEX root = main.tex
\section{Introduction}

Methods for scientific computing are traditionally implemented in specialized software packages assisting the statistician in all facets of the data analysis process. A single product typically includes a wealth of functionality to interactively manage, explore and analyse data, and often much more. Some products such as \R or \texttt{STATA} are optimized for use via a command line interface (\CLI) whereas others such as \SPSS focus mainly on the graphical user interface (\GUI). However increasingly many users and organisations wish to integrate statistical methods into third party software. Rather than performing data analysis in a statistical environment, methods for analyzing and visualizing data get incorporated into pipelines, web applications and big data infrastructures. This way of doing data analysis requires a different approach to statistical software which puts more emphasis on modularity, interoperability and programmable interfaces rather than \UI interaction. Throughout this paper we refer to this approach to statistical software as \emph{embedded scientific computing}.

The most challenging aspect of embedded scientific computing is specification of the interface. Clients integrating analysis components require an API which defines statistical operations independently of any programming language. 

component based data analysis units
seperation of concerns
resusable modules
what are the concerns of data analysis? 
explore domain logic
challenges in transitioning from a \UI to \API paradigm
work towards interface definition

Rather than focusing on technical problems, our approach puts heavy emphasis on exploring the domain logic of the field along the lines of \citep{evans2004domain}. Scalable components require meticulous separation of concerns, and a significant share of this work is devoted to describing the concerns of scientific computing. To this end, our journey starts with an extensive review of the purpose, problems and practices of data analysis, and the implications for embedded components. The goal is to study the conceptual model of scientific computing and carefully separate it from implementation or application. This provides the requirements for a system that facilitates embedded scientific computing, and sets the stage for an interface definition.





\subsection{The need for abstraction}

Previous work in embedded scientific computing has mostly been limited to low-level tools for connecting statistical software to general purpose environments. For \R, bindings and bridges are available to execute an \R script or process from inside most popular languages. For example, \texttt{JRI} \citep{rjava}, \texttt{RInside} \citep{eddelbuettel2011rcpp}, \texttt{rpy2} \citep{gautier2008rpy2} or \texttt{RinRuby} \citep{dahl2008rinruby} can be used to call \R from respectively \texttt{Java}, \texttt{C++}, \texttt{Python} or \texttt{Ruby}. \cite{heiberger2009r} provide a set of tools to execute \R code from \texttt{DCOM} clients on Windows, mostly to support running \R in Microsoft Excel. The Apache2 module \texttt{rApache} (\texttt{mod\_R}) makes it possible to run R scripts from the Apache2 web server \citep{horner2013rapache}. The \texttt{littler} program provides hash-bang capability for \R, as well as simple command-line and piping use on \texttt{unix} \citep{littler}. Finally, \texttt{Rserve} is \texttt{TCP/IP} server which provides low level access to an R session over a socket \citep{urbanek2013rserve}. 

However even though these tools have been available for several years, they have not been able to facilitate the big break through of \R as a ubiquitous statistical engine. Given the enourmous demand for analysis and visualization these days, the adoption of \R for embedded scientific computing is actually quite underwhelming. In my experience, the primary cause for the limited success is that low-level bridges are difficult to implement, do not scale very well, and leave the most challenging problems unsolved. Substantial plumbing and expertise of \R internals is required for building actual applications on these tools. Clients are supposed to generate and push \R syntax, make sense of \R's internal \texttt{C} structures and write their own framework for managing jobs, graphics, security, data interchange, etc. In practice this is very difficult, results in messy code, and often leads to unstable applications when important issues were not fully taken care of. The tools are also impractical from a human point of view. Building a web application with for example \texttt{JRI} requires a web developer that is also an expert in \R and \texttt{Java}. Because \R is such a domain specific language, this combination of skills is very rare. 

What is needed to scale up embedded scientific computing is a system that decouples the application layer from the computational back-end, similar to how e.g. \texttt{SQL} separates the application and database. A well defined separation of concerns allows for independent development of client and server components, by different people with different background and expertise. Idealy, the system interface should describe domain logic of scientific computing and abstract implementation details as much as possible. This gives clients a universal point of interaction to perform data operations without any specialized knowledge about the computational language or infrastructure. At the same time, statisticians can expose analysis methods for use in applications without having to learn web development or other tools. Careful isolation of components along the lines of domain logic would allow developers to focus on their expertise using their tools of choice. This vision forms the starting point of this research. 


\subsection{The \OpenCPU system}

The \OpenCPU system is used as a reference implementation throughout this paper. \OpenCPU defines an \HTTP \API that builds on \emph{The R Project for Statistical Computing} \citep{R}, for short: \R. The \R language is the obvious candidate for a first implementation of this kind. It is currently the most popular open source software for data analysis and considered by many statisticians as the \emph{de facto} standard of data analysis. The huge \R community provides both the tools and use-cases needed to develop and experiment with this new approach to scientific computing. Currently only \R has the required scale and foundations to really put our ideas to the test. However we want to stress that even though our research and the \OpenCPU system are inspired by and tailored to the way things work in \R, the approach should generalize quite naturally to other computational back-ends. We try to capture the general logic of data analysis rather than that of a particular language, but in practice it is very difficult to study or discuss software without an implementation. The \OpenCPU system illustrates what an abstracted interface to scientific computing could look like. The implementation allows us and others to put this new approach into practice and get first hand experience with the problems and opportunities in this unexplored field. 


%\subsection{Reference Implementation}

As part of the research, two reference implementations were developed. The \R package \texttt{opencpu} builds on the \texttt{httpuv} web server (another \R package) to implement a \emph{single-user server} which runs within an interactive R session on any platform. The \emph{cloud server} is a multi-user implementation based on \texttt{Linux} and \texttt{rApache}. The latter yields better performance and has advanced security and configuration options, however it requires a dedicated Ubuntu server. One major difference between these implementations is how they handle concurrency. Because \R is single threaded, \texttt{httpuv} handles only one request at a time. Additional incoming requests are automatically queued and executed in succession using the same process. The cloud server on the other hand takes advantage of multi-processing in the Apache2 web server to handle concurrency. This implementation uses forks of the R process to serve concurrent requests immediately with little performance overhead. 

The differences between the cloud server and single user server are invisible to clients. The OpenCPU API provides a standard interface to either implementation. Other than varying performance, applications will behave the same regardless of which server is used. This shows the strenght of the system: applications can be developed locally using the single user server and published later on a shared high-performance cloud server.


\subsection{Less is more}

\begin{quote}
\emph{``perfection is reached not when there is nothing left to add, but when there is nothing left to take away''} --- Antoine de Saint-Exupéry
\end{quote}

\vspace{8pt}

%KISS (\emph{``Keep it simple, stupid''}) is a popular ancronym coined in the 1960s by the U.S. Navy \citep{victor2007concise} for the design principle stating that systems work best if they are kept simple rather than made complicated. Some other frequently cited testimonials of similar observations include \emph{``less is more''} (Mies van der Rohe), \emph{``Simplicity is the ultimate sophistication''} (Leonardo da Vinci), and \emph{``perfection is reached not when there is nothing left to add, but when there is nothing left to take away''} (Antoine de Saint-Exupéry). 

\noindent Management of complexity is one of the most challenging aspects of modern software development. When pressure and temptation to expand functionality is not balanced by a constant strive for simplicity, a project is heading for failure. Each newly introduced feature demands additional testing, maintenance and support down the road, which can rapidly turn into a sinkhole of developer time. Moreover costs, overhead and limitations introduced by unwanted stuff make bloated software less attractive. Also programs gets increasingly difficult to learn and understand if important principles are masked by smoke screens, bells and whistles of vaguely related bonus features. Therefore finding a minimal generalizable solution to address a particular problem or market is key to software simplicity. However this is rarely easy. It requires careful deliberation of the role, scope and priorities of a system; of what it \emph{is} and what it should or should not do. Incomplete solutions that leave important problems unsolved are unreliable and impractical. However components that are overly involved might conflict with other pieces, and complicate the system. The quest for a clean and elegant design often consists of many iterations of adding, removing, abstracting, rethinking and refactoring the system.

For OpenCPU a lot of thought has gone into what are the fundamental building blocks of embedded scientific computing. The system should solve recurring problems and give structure to applications without imposing unnecessary limitations. The OpenCPU API sets out important guidelines on what is considered the responsibility of respectively the developer (client-side), statistician (server-side) and system itself (middle layer). For example, all \R functions and scripts accessible through the OpenCPU system. However, the API only specifies  procedures for remotely \emph{calling} such functions and \emph{retrieving} results. The specification does not mention any details on the behavior and implementation of functions or which data is exchanged. Other concerns such as authentication can be solved on the layer of the application protocol (i.e. HTTP). Hence even though useful, these are not fundamental to scientific computing specifically and therefore need no specification in our API. Finally the system has some meta-functionality such as reproducibility mentioned earlier. It is tempting to include more goodies such as permanent storage, system administration or front-end widgets. However, such features are not specific to scientific computing and can easily be realized as separate, independent software pieces. Therefore they do not need to be part of the specification either. 

OpenCPU is what remains after all non essential parts are taken away from the API. It defines a middle layer that addresses the core problems of embedded scientific computing, and nothing else. It specifies a system for management of resources and remote function calls over HTTP that isolates scientific computing from other parts of the system. It prescribes several data interchange formats, but no specific data types or structures. The system is intended to be used \emph{in conjunction} with other software such as database, web framework or administration tools. It is a powerful building block for stacks and applications with embedded analysis or visualization. 

\subsection{History of OpenCPU}

The OpenCPU specification outlined in this paper was not drafted overnight, but evolved as many iterations of trial and error. Initial inspirations were drawn from recurring problems in developing various \R web applications with \texttt{rApache}. Experiences from these applications revelealed fundamental challenges and domain logic of embedded scientific computing. The accumulated knowledge shaped a vision on what a system facilitating this entails that would eventually turn into OpenCPU.

After about a year of internal development, the first public version of OpenCPU appeared in August 2011. These alpha and beta versions were picked up by several early adopters, both from industry and academia, some of which are still in production today. The problems and critisim generated by these versions provided great feedback and revealed some fundamental design problems. At the same time exciting developments were going on in the \R community, in particular the rise of R-studio and introduction of influential \R packages \texttt{knitr}, \texttt{evaluate} and \texttt{httpuv}. These technologies provided more flexible and reliable foundations for OpenCPU than what was available than before. After a redesign of the API and a complete rewrite of the code, OpenCPU version  1.0 was released in August 2013. This version emphasizes simplicity and scalability by removing all non-essential functionality. What remains is an API that captures the core domain logic of embedded scientific computing. By making use of native features found in HTTP, the this version is flexible and extensible without getting bloated or overly complex.


%For this reason the paper discusses concerns of scientific computing mostly in the context and terminology of \R.  Therefore we rely on \R to illustrate and exemplify the concepts of scientific computing. For example, the conventional container format for namespacing a collection of data and functions in \R is a \emph{package}. Other products might use different mechanics and jargon for combining and publishing a set of objects. However any system needs some notion of namespaces and dependencies analogous to packages in \R. For sake of simplicity this paper discusses such concepts mostly from the way they take shape in \R. 
